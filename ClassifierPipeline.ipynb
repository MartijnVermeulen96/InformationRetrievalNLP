{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import twokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from FeatureFunctions import getfeatures\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading train and test files\n",
    "\n",
    "datafile = \"datasets/train/SemEval2018-T3-train-taskA_emoji.txt\"\n",
    "# datafile2 = \"datasets/train/SemEval2018-T3-train-taskA_emoji_ironyHashtags.txt\"\n",
    "trainingdata = pd.read_csv(datafile, sep = \"\\t\", header=0)\n",
    "\n",
    "type(trainingdata['Tweet text'])\n",
    "trainingdata = trainingdata[['Label','Tweet text']]\n",
    "train_tweets = trainingdata['Tweet text']\n",
    "\n",
    "testfile = 'datasets/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt'\n",
    "testdata = pd.read_csv(testfile, sep=\"\\t\", header=0)\n",
    "testdata = testdata[['Label','Tweet text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating 50-50% balance\n",
    "## Train\n",
    "amount_nonirony_train = sum(trainingdata[\"Label\"] == 0)\n",
    "amount_irony_train = sum(trainingdata[\"Label\"] == 1)\n",
    "amount_train_amount = min(amount_nonirony_train, amount_irony_train)\n",
    "\n",
    "resulting_nonirony_train = trainingdata[trainingdata[\"Label\"] == 0].sample(amount_train_amount)\n",
    "resulting_irony_train = trainingdata[trainingdata[\"Label\"] == 1].sample(amount_train_amount)\n",
    "\n",
    "trainingdata = resulting_nonirony_train.append(resulting_irony_train, ignore_index=True)\n",
    "\n",
    "## Test\n",
    "amount_nonirony_test = sum(testdata[\"Label\"] == 0)\n",
    "amount_irony_test = sum(testdata[\"Label\"] == 1)\n",
    "amount_test_amount = min(amount_nonirony_test, amount_irony_test)\n",
    "\n",
    "resulting_nonirony_test = testdata[testdata[\"Label\"] == 0].sample(amount_test_amount)\n",
    "resulting_irony_test = testdata[testdata[\"Label\"] == 1].sample(amount_test_amount)\n",
    "\n",
    "testdata = resulting_nonirony_test.append(resulting_irony_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get lexical features\n",
    "# training_features\n",
    "lexical_training_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(trainingdata, 'Tweet text')\n",
    "x_small = lexical_training_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "x_lexical = np.array(lexical_training_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "x_lexical = np.hstack((x_small, x_lexical))\n",
    "\n",
    "# train_bow = lexical_training_features['UnigramVector'].values.tolist()\n",
    "\n",
    "# test_features\n",
    "lexical_test_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(testdata, 'Tweet text', unicount_vect, bicount_vect, tricount_vect, fourcount_vect)\n",
    "test_x_small = lexical_test_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "test_lexical_x = np.array(lexical_test_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "test_lexical_x = np.hstack((test_x_small, test_lexical_x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = np.array(lexical_training_features['UnigramVector'].values.tolist())\n",
    "test_bow = np.array(lexical_test_features['UnigramVector'].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get sentiment features\n",
    "train_sentiment_x = getfeatures.getaffinfeats(trainingdata['Tweet text'])\n",
    "test_sentiment_x = getfeatures.getaffinfeats(testdata['Tweet text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combining features\n",
    "\n",
    "whichtype = input(\"Which type to test: \")\n",
    "\n",
    "if whichtype == 1:\n",
    "    final_train_x = train_bow\n",
    "    final_test_x = test_bow\n",
    "\n",
    "elif whichtype == 6\n",
    "    final_train_x = np.hstack((train_sentiment_x, x_lexical))\n",
    "    final_test_x = np.hstack((test_sentiment_x, test_lexical_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the classifier\n",
    "import sys\n",
    "sys.path.insert(0, 'libsvm')\n",
    "\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 92.3987% (3513/3802) (classification)\n"
     ]
    }
   ],
   "source": [
    "### Train and get train error\n",
    "y = trainingdata['Label'].tolist()\n",
    "prob  = svm_problem(y, final_train_x)\n",
    "param = svm_parameter('-t 2 -c 8 -g ' + str(2**-11))\n",
    "m = svm_train(prob, param)\n",
    "p_label, p_acc, p_val = svm_predict(y, final_train_x, m)\n",
    "ACC, MSE, SCC = evaluations(y, p_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 66.2379% (412/622) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-irony     0.6563    0.6817    0.6688       311\n",
      "       Irony     0.6689    0.6431    0.6557       311\n",
      "\n",
      "   micro avg     0.6624    0.6624    0.6624       622\n",
      "   macro avg     0.6626    0.6624    0.6623       622\n",
      "weighted avg     0.6626    0.6624    0.6623       622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get the test\n",
    "test_y = testdata['Label'].tolist()\n",
    "\n",
    "test_p_label, test_p_acc, test_p_val = svm_predict(test_y, final_test_x, m)\n",
    "test_ACC, test_MSE, test_SCC = evaluations(test_y, test_p_label)\n",
    "print(sklearn.metrics.classification_report(test_y, test_p_label, target_names=[\"Non-irony\", \"Irony\"], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
