{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import twokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from FeatureFunctions import getfeatures\n",
    "from FeatureFunctions import helper\n",
    "from sklearn.externals import joblib\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import sklearn\n",
    "import csv\n",
    "\n",
    "\n",
    "### Import the classifier\n",
    "import sys\n",
    "sys.path.insert(0, 'libsvm')\n",
    "\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichtask = input(\"Which task to you want to do (A/B): \")\n",
    "whichtask = \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading train and test files\n",
    "\n",
    "datafile = \"datasets/train/SemEval2018-T3-train-task\"+whichtask+\"_emoji.txt\"\n",
    "trainingdata = pd.read_csv(datafile, delimiter = \"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "trainingdata = trainingdata[['Label','Tweet text']]\n",
    "\n",
    "testfile = 'datasets/goldtest_Task'+whichtask+'/SemEval2018-T3_gold_test_task'+whichtask+'_emoji.txt'\n",
    "testdata = pd.read_csv(testfile, sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "testdata = testdata[['Label','Tweet text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get lexical features\n",
    "# training_features\n",
    "lexical_training_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(trainingdata, 'Tweet text')\n",
    "x_small = lexical_training_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "x_lexical = np.array(lexical_training_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "x_lexical = np.hstack((x_small, x_lexical))\n",
    "\n",
    "# train_bow = lexical_training_features['UnigramVector'].values.tolist()\n",
    "\n",
    "# test_features\n",
    "lexical_test_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(testdata, 'Tweet text', unicount_vect, bicount_vect, tricount_vect, fourcount_vect)\n",
    "test_x_small = lexical_test_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "test_lexical_x = np.array(lexical_test_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "test_lexical_x = np.hstack((test_x_small, test_lexical_x))\n",
    "\n",
    "train_bow = np.array(lexical_training_features['UnigramVector'].values.tolist())\n",
    "test_bow = np.array(lexical_test_features['UnigramVector'].values.tolist())\n",
    "\n",
    "lexical_training_features = []\n",
    "lexical_test_features = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the classifier\n",
    "kmeansmodel = joblib.load(\"FeatureFunctions/kmeansclass.model\")\n",
    "w2vmodel = Word2Vec.load(\"FeatureFunctions/word2vec.model\")\n",
    "\n",
    "train_bags = helper.allbags(trainingdata['Tweet text'])\n",
    "word2vecfeatures_train = helper.onehotwordclusters(train_bags, kmeansmodel, w2vmodel)\n",
    "\n",
    "test_bags = helper.allbags(testdata['Tweet text'])\n",
    "word2vecfeatures_test = helper.onehotwordclusters(test_bags, kmeansmodel, w2vmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vecfeatures_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+ only.\n",
    "reload(getfeatures)\n",
    "\n",
    "### Get sentiment features\n",
    "train_sentiment_x = getfeatures.getaffinfeats(trainingdata['Tweet text'])\n",
    "test_sentiment_x = getfeatures.getaffinfeats(testdata['Tweet text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getindices(trainingdata, train_split=9, test_split=1):\n",
    "    ### Creating 50-50% balance\n",
    "    ## Train\n",
    "    \n",
    "    ### Count the amount of samples\n",
    "    amount_nonirony_train = sum(trainingdata[\"Label\"] == 0)\n",
    "    amount_irony_train = sum(trainingdata[\"Label\"] > 0)\n",
    "    amount_train_amount = min(amount_nonirony_train, amount_irony_train)\n",
    "    \n",
    "    \n",
    "\n",
    "    ### Sample indices\n",
    "    nonirony_index = trainingdata[trainingdata[\"Label\"] == 0].index.to_series()\n",
    "    irony_index = trainingdata[trainingdata[\"Label\"] > 0].index.to_series()\n",
    "\n",
    "    total_nonirony_samples = nonirony_index.sample(amount_train_amount).tolist()\n",
    "    total_irony_samples = irony_index.sample(amount_train_amount).tolist()\n",
    "    \n",
    "    train_amount = round(amount_train_amount / (train_split + test_split) * train_split)    \n",
    "    test_amount = round(amount_train_amount / (train_split + test_split) * test_split)    \n",
    "    \n",
    "    print(len(total_nonirony_samples))\n",
    "    print(len(total_irony_samples))\n",
    "\n",
    "    resulting_train_index = total_nonirony_samples[:train_amount] + total_irony_samples[:train_amount]\n",
    "    resulting_test_index = total_nonirony_samples[train_amount+1:] + total_irony_samples[train_amount+1:]\n",
    "\n",
    "    return resulting_train_index, resulting_test_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations: 10\n",
      "Do you want to use a heldout set (1=heldout, 0=crossvalidate): 0\n",
      "Which type to test: 5\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.6109% (1670/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 50.2625% (383/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5045    0.2966    0.3736       381\n",
      "           1     0.5019    0.7087    0.5876       381\n",
      "\n",
      "   micro avg     0.5026    0.5026    0.5026       762\n",
      "   macro avg     0.5032    0.5026    0.4806       762\n",
      "weighted avg     0.5032    0.5026    0.4806       762\n",
      "\n",
      "Precision: 0.5018587360594795\n",
      "Recall: 0.7086614173228346\n",
      "F1-score: 0.5875952121871598\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          113  268  381\n",
      "1          111  270  381\n",
      "All        224  538  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 2/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.0876% (1654/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 53.5433% (408/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5389    0.4908    0.5137       381\n",
      "           1     0.5325    0.5801    0.5553       381\n",
      "\n",
      "   micro avg     0.5354    0.5354    0.5354       762\n",
      "   macro avg     0.5357    0.5354    0.5345       762\n",
      "weighted avg     0.5357    0.5354    0.5345       762\n",
      "\n",
      "Precision: 0.5325301204819277\n",
      "Recall: 0.5800524934383202\n",
      "F1-score: 0.5552763819095478\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          187  194  381\n",
      "1          160  221  381\n",
      "All        347  415  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 3/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.8725% (1678/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 51.8373% (395/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5343    0.2861    0.3726       381\n",
      "           1     0.5125    0.7507    0.6092       381\n",
      "\n",
      "   micro avg     0.5184    0.5184    0.5184       762\n",
      "   macro avg     0.5234    0.5184    0.4909       762\n",
      "weighted avg     0.5234    0.5184    0.4909       762\n",
      "\n",
      "Precision: 0.5125448028673835\n",
      "Recall: 0.7506561679790026\n",
      "F1-score: 0.6091586794462193\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          109  272  381\n",
      "1           95  286  381\n",
      "All        204  558  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 4/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.5782% (1669/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 53.2808% (406/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5740    0.2546    0.3527       381\n",
      "           1     0.5211    0.8110    0.6345       381\n",
      "\n",
      "   micro avg     0.5328    0.5328    0.5328       762\n",
      "   macro avg     0.5475    0.5328    0.4936       762\n",
      "weighted avg     0.5475    0.5328    0.4936       762\n",
      "\n",
      "Precision: 0.521079258010118\n",
      "Recall: 0.8110236220472441\n",
      "F1-score: 0.6344969199178645\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0           97  284  381\n",
      "1           72  309  381\n",
      "All        169  593  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 5/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.5782% (1669/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 53.8058% (410/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5687    0.3150    0.4054       381\n",
      "           1     0.5263    0.7612    0.6223       381\n",
      "\n",
      "   micro avg     0.5381    0.5381    0.5381       762\n",
      "   macro avg     0.5475    0.5381    0.5139       762\n",
      "weighted avg     0.5475    0.5381    0.5139       762\n",
      "\n",
      "Precision: 0.5263157894736842\n",
      "Recall: 0.7611548556430446\n",
      "F1-score: 0.6223175965665235\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          120  261  381\n",
      "1           91  290  381\n",
      "All        211  551  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 6/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.0222% (1652/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 56.2992% (429/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6154    0.3360    0.4346       381\n",
      "           1     0.5433    0.7900    0.6439       381\n",
      "\n",
      "   micro avg     0.5630    0.5630    0.5630       762\n",
      "   macro avg     0.5794    0.5630    0.5392       762\n",
      "weighted avg     0.5794    0.5630    0.5392       762\n",
      "\n",
      "Precision: 0.5433212996389891\n",
      "Recall: 0.7900262467191601\n",
      "F1-score: 0.6438502673796791\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          128  253  381\n",
      "1           80  301  381\n",
      "All        208  554  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 7/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.3819% (1663/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 51.5748% (393/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5323    0.2598    0.3492       381\n",
      "           1     0.5104    0.7717    0.6144       381\n",
      "\n",
      "   micro avg     0.5157    0.5157    0.5157       762\n",
      "   macro avg     0.5213    0.5157    0.4818       762\n",
      "weighted avg     0.5213    0.5157    0.4818       762\n",
      "\n",
      "Precision: 0.5104166666666666\n",
      "Recall: 0.7716535433070866\n",
      "F1-score: 0.6144200626959248\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0           99  282  381\n",
      "1           87  294  381\n",
      "All        186  576  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 8/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.4474% (1665/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 54.1995% (413/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5762    0.3176    0.4095       381\n",
      "           1     0.5290    0.7664    0.6259       381\n",
      "\n",
      "   micro avg     0.5420    0.5420    0.5420       762\n",
      "   macro avg     0.5526    0.5420    0.5177       762\n",
      "weighted avg     0.5526    0.5420    0.5177       762\n",
      "\n",
      "Precision: 0.5289855072463768\n",
      "Recall: 0.7664041994750657\n",
      "F1-score: 0.6259378349410505\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          121  260  381\n",
      "1           89  292  381\n",
      "All        210  552  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 9/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 55.4938% (1697/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 51.9685% (396/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5241    0.4278    0.4711       381\n",
      "           1     0.5166    0.6115    0.5601       381\n",
      "\n",
      "   micro avg     0.5197    0.5197    0.5197       762\n",
      "   macro avg     0.5204    0.5197    0.5156       762\n",
      "weighted avg     0.5204    0.5197    0.5156       762\n",
      "\n",
      "Precision: 0.516629711751663\n",
      "Recall: 0.6115485564304461\n",
      "F1-score: 0.5600961538461539\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          163  218  381\n",
      "1          148  233  381\n",
      "All        311  451  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 10/10\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 2000)\n",
      "\n",
      "Accuracy = 54.3819% (1663/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 2000)\n",
      "\n",
      "Accuracy = 52.7559% (402/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5335    0.4383    0.4813       381\n",
      "           1     0.5234    0.6168    0.5663       381\n",
      "\n",
      "   micro avg     0.5276    0.5276    0.5276       762\n",
      "   macro avg     0.5285    0.5276    0.5238       762\n",
      "weighted avg     0.5285    0.5276    0.5238       762\n",
      "\n",
      "Precision: 0.5233853006681515\n",
      "Recall: 0.6167979002624672\n",
      "F1-score: 0.5662650602409638\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          167  214  381\n",
      "1          146  235  381\n",
      "All        313  449  762\n",
      "Word2Vec results with 10 iterations: \n",
      "52.95 ±3.18 -50.26 +56.30\t52.17 ±2.27 -50.19 +54.33\t71.68 ±15.83 -58.01 +81.10\t60.19 ±6.14 -55.53 +64.39\tWord2Vec results\n"
     ]
    }
   ],
   "source": [
    "### Combining features\n",
    "iterations = int(input(\"How many iterations: \"))\n",
    "heldoutset = int(input(\"Do you want to use a heldout set (1=heldout, 0=crossvalidate): \"))\n",
    "whichtype = int(input(\"Which type to test: \"))\n",
    "results = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    print()\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(\"Iteration \" + str(i+1) + \"/\" + str(iterations))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "    ### Selecting the actual training samples (Do some crossover here )\n",
    "    if heldoutset:\n",
    "        trainingdata_result = trainingdata\n",
    "        testdata_result = testdata\n",
    "        resulting_train_index = range(len(trainingdata))\n",
    "        resulting_test_index = range(len(testdata_result))\n",
    "    else:\n",
    "        ## Crossvalidating\n",
    "        resulting_train_index, resulting_test_index = getindices(trainingdata, 4, 1) # Get 80-20\n",
    "        trainingdata_result = trainingdata.loc[resulting_train_index]\n",
    "        testdata_result = trainingdata.loc[resulting_test_index]\n",
    "    \n",
    "    final_train_x = []\n",
    "    final_test_x = []\n",
    "\n",
    "    if heldoutset:\n",
    "        ## BoW\n",
    "        if whichtype == 1:\n",
    "            final_train_x = train_bow[resulting_train_index]\n",
    "            final_test_x = test_bow[resulting_test_index]\n",
    "        ## Lexical\n",
    "        if whichtype == 2:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((test_bow[resulting_test_index], test_lexical_x[resulting_test_index]))\n",
    "        ## Sentiment\n",
    "        if whichtype == 4:\n",
    "            final_train_x = train_sentiment_x[resulting_train_index]\n",
    "            final_test_x = test_sentiment_x[resulting_test_index]\n",
    "        ## Word2Vec\n",
    "        if whichtype == 5:\n",
    "            final_train_x = word2vecfeatures_train[resulting_train_index]\n",
    "            final_test_x = word2vecfeatures_test[resulting_test_index]\n",
    "        ## Combined\n",
    "        if whichtype == 6:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], train_sentiment_x[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((test_bow[resulting_test_index], test_sentiment_x[resulting_test_index], test_lexical_x[resulting_test_index]))\n",
    "    else:\n",
    "        ## Crossvalidating\n",
    "        ## BoW\n",
    "        if whichtype == 1:\n",
    "            final_train_x = train_bow[resulting_train_index]\n",
    "            final_test_x = train_bow[resulting_test_index]\n",
    "        ## Lexical\n",
    "        if whichtype == 2:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((train_bow[resulting_test_index], x_lexical[resulting_test_index]))\n",
    "        ## Sentiment\n",
    "        if whichtype == 4:\n",
    "            final_train_x = train_sentiment_x[resulting_train_index]\n",
    "            final_test_x = train_sentiment_x[resulting_test_index]\n",
    "        ## Word2Vec\n",
    "        if whichtype == 5:\n",
    "            final_train_x = word2vecfeatures_train[resulting_train_index]\n",
    "            final_test_x = word2vecfeatures_train[resulting_test_index]\n",
    "        ## Combined\n",
    "        if whichtype == 6:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], train_sentiment_x[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((train_bow[resulting_test_index], train_sentiment_x[resulting_test_index], x_lexical[resulting_test_index]))\n",
    "\n",
    "\n",
    "    print(\"###########\")\n",
    "    print(\"Training:\")\n",
    "    print(final_train_x.shape)\n",
    "    print()\n",
    "    ### Train and get train error\n",
    "    y = trainingdata_result['Label'].tolist()\n",
    "    prob  = svm_problem(y, final_train_x)\n",
    "    param = svm_parameter('-t 2 -c 8 -g ' + str(2**-11))\n",
    "    m = svm_train(prob, param)\n",
    "    p_label, p_acc, p_val = svm_predict(y, final_train_x, m)\n",
    "    ACC, MSE, SCC = evaluations(y, p_label)\n",
    "\n",
    "    print()\n",
    "    print(\"###########\")\n",
    "    print(\"Testing:\")\n",
    "    print(final_test_x.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "    ### Get the test\n",
    "    test_y = testdata_result['Label'].tolist()\n",
    "\n",
    "    test_p_label, test_p_acc, test_p_val = svm_predict(test_y, final_test_x, m)\n",
    "    test_ACC, test_MSE, test_SCC = evaluations(test_y, test_p_label)\n",
    "    print(sklearn.metrics.classification_report(test_y, test_p_label, digits=4))\n",
    "\n",
    "    if whichtask == \"A\":\n",
    "        p, r, f = helper.precision_recall_fscore(test_y, test_p_label, beta=1, labels=[0,1], pos_label=1)\n",
    "    elif whichtask == \"B\":\n",
    "        p, r, f = helper.precision_recall_fscore(test_y, test_p_label, beta=1, labels=[0,1,2,3])\n",
    "\n",
    "    print(\"Precision: \" + str(p))\n",
    "    print(\"Recall: \" + str(r))\n",
    "    print(\"F1-score: \" + str(f))\n",
    "    \n",
    "    results.append([test_ACC, p*100, r*100, f*100])\n",
    "\n",
    "    y_actu = pd.Series(test_y, name='Actual')\n",
    "    y_pred = pd.Series(test_p_label, name='Predicted')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(df_confusion)\n",
    "    \n",
    "\n",
    "### Write output\n",
    "mean_results = np.mean(np.array(results), axis=0)\n",
    "std_results = 2*np.std(np.array(results), axis=0)\n",
    "min_results = np.min(np.array(results), axis=0)\n",
    "max_results = np.max(np.array(results), axis=0)\n",
    "\n",
    "if whichtype == 1:\n",
    "    sys.stdout.write(\"BoW results\")\n",
    "elif whichtype == 2:\n",
    "    sys.stdout.write(\"Lexical results\")\n",
    "elif whichtype == 4:\n",
    "    sys.stdout.write(\"Semantic results\")\n",
    "elif whichtype == 5:\n",
    "    sys.stdout.write(\"Word2Vec results\")\n",
    "elif whichtype == 6:\n",
    "    sys.stdout.write(\"Combined results\")\n",
    "    \n",
    "### stdout\n",
    "print(\" with \" + str(iterations) + \" iterations: \")\n",
    "for result_i in range(len(mean_results)):\n",
    "    sys.stdout.write(\"{:.2f}\".format(mean_results[result_i]))\n",
    "    sys.stdout.write(\" ±\" + \"{:.2f}\".format(std_results[result_i]))\n",
    "    sys.stdout.write(\" -\" + \"{:.2f}\".format(min_results[result_i]))\n",
    "    sys.stdout.write(\" +\" + \"{:.2f}\".format(max_results[result_i]) + \"\\t\")\n",
    "    \n",
    "    \n",
    "### file out\n",
    "with open(\"Results/resultswrite.txt\", \"a\") as outfile:\n",
    "    if whichtype == 1:\n",
    "        outfile.write(\"BoW results\")\n",
    "    elif whichtype == 2:\n",
    "        outfile.write(\"Lexical results\")\n",
    "    elif whichtype == 4:\n",
    "        outfile.write(\"Semantic results\")\n",
    "    elif whichtype == 5:\n",
    "        outfile.write(\"Word2Vec results\")\n",
    "    elif whichtype == 6:\n",
    "        outfile.write(\"Combined results\")\n",
    "\n",
    "    ### stdout\n",
    "    outfile.write(\" with \" + str(iterations) + \" iterations: \\n\")\n",
    "    for result_i in range(len(mean_results)):\n",
    "        outfile.write(\"{:.2f}\".format(mean_results[result_i]))\n",
    "        outfile.write(\" ±\" + \"{:.2f}\".format(std_results[result_i]))\n",
    "        outfile.write(\" -\" + \"{:.2f}\".format(min_results[result_i]))\n",
    "        outfile.write(\" +\" + \"{:.2f}\".format(max_results[result_i]) + \"\\t\")\n",
    "        \n",
    "    outfile.write(\"\\n\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if heldoutset:\n",
    "    ### Evaluate results per category (Sectie 5.2)\n",
    "\n",
    "    testfileB = 'datasets/goldtest_TaskB/SemEval2018-T3_gold_test_taskB_emoji.txt'\n",
    "    testdataB = pd.read_csv(testfileB, sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "    testdataB = testdataB[['Label','Tweet text']]\n",
    "\n",
    "    ## Getting category information\n",
    "    used_testB = testdataB.loc[resulting_test_index]\n",
    "    used_testB.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    ## Nonirony\n",
    "    print(\"--- 0 Not ironic\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 0) & (y_pred == 0))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 0))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 1\n",
    "    print(\"--- 1 Ironic by clash\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 1) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 1))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 2\n",
    "    print(\"--- 2 Situational irony\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 2) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 2))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 3\n",
    "    print(\"--- 3 Other irony\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 3) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 3))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata_result = trainingdata.loc[resulting_train_index]\n",
    "testdata_result = trainingdata.loc[resulting_test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_result.index\n",
    "sum(testdata[\"Label\"] == 1) / len(testdata_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([1 for yi in y if yi == 0]))\n",
    "print(sum([1 for yi in y if yi == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
