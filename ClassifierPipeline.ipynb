{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import twokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from FeatureFunctions import getfeatures\n",
    "from FeatureFunctions import helper\n",
    "from sklearn.externals import joblib\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import sklearn\n",
    "import csv\n",
    "\n",
    "\n",
    "### Import the classifier\n",
    "import sys\n",
    "sys.path.insert(0, 'libsvm')\n",
    "\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichtask = input(\"Which task to you want to do (A/B): \")\n",
    "whichtask = \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading train and test files\n",
    "\n",
    "datafile = \"datasets/train/SemEval2018-T3-train-task\"+whichtask+\"_emoji.txt\"\n",
    "trainingdata = pd.read_csv(datafile, delimiter = \"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "trainingdata = trainingdata[['Label','Tweet text']]\n",
    "\n",
    "testfile = 'datasets/goldtest_Task'+whichtask+'/SemEval2018-T3_gold_test_task'+whichtask+'_emoji.txt'\n",
    "testdata = pd.read_csv(testfile, sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "testdata = testdata[['Label','Tweet text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get lexical features\n",
    "# training_features\n",
    "lexical_training_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(trainingdata, 'Tweet text')\n",
    "x_small = lexical_training_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "x_lexical = np.array(lexical_training_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "x_lexical = np.hstack((x_small, x_lexical))\n",
    "\n",
    "# train_bow = lexical_training_features['UnigramVector'].values.tolist()\n",
    "\n",
    "# test_features\n",
    "lexical_test_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(testdata, 'Tweet text', unicount_vect, bicount_vect, tricount_vect, fourcount_vect)\n",
    "test_x_small = lexical_test_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "test_lexical_x = np.array(lexical_test_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "test_lexical_x = np.hstack((test_x_small, test_lexical_x))\n",
    "\n",
    "train_bow = np.array(lexical_training_features['UnigramVector'].values.tolist())\n",
    "test_bow = np.array(lexical_test_features['UnigramVector'].values.tolist())\n",
    "\n",
    "lexical_training_features = []\n",
    "lexical_test_features = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the classifier\n",
    "kmeansmodel = joblib.load(\"FeatureFunctions/kmeansclass.model\")\n",
    "w2vmodel = Word2Vec.load(\"FeatureFunctions/word2vec.model\")\n",
    "\n",
    "train_bags = helper.allbags(trainingdata['Tweet text'])\n",
    "word2vecfeatures_train = helper.onehotwordclusters(train_bags, kmeansmodel, w2vmodel).astype(int)\n",
    "\n",
    "test_bags = helper.allbags(testdata['Tweet text'])\n",
    "word2vecfeatures_test = helper.onehotwordclusters(test_bags, kmeansmodel, w2vmodel).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+ only.\n",
    "reload(getfeatures)\n",
    "\n",
    "### Get sentiment features\n",
    "train_sentiment_x = getfeatures.getaffinfeats(trainingdata['Tweet text'])\n",
    "test_sentiment_x = getfeatures.getaffinfeats(testdata['Tweet text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations: 1\n",
      "Do you want to use a heldout set (1=heldout, 0=crossvalidate): 1\n",
      "Which type to test: 6\n",
      "Should I use the nu-svc (1)?: 0\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1/1\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###########\n",
      "Training:\n",
      "(3834, 65417)\n",
      "\n",
      "Accuracy = 50.1565% (1923/3834) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(784, 65417)\n",
      "\n",
      "Accuracy = 60.3316% (473/784) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6033    1.0000    0.7526       473\n",
      "           1     0.0000    0.0000    0.0000       311\n",
      "\n",
      "   micro avg     0.6033    0.6033    0.6033       784\n",
      "   macro avg     0.3017    0.5000    0.3763       784\n",
      "weighted avg     0.3640    0.6033    0.4540       784\n",
      "\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "\n",
      "\n",
      "Predicted  0.0  All\n",
      "Actual             \n",
      "0          473  473\n",
      "1          311  311\n",
      "All        784  784\n",
      "Combined results with 1 iterations: \n",
      "60.33 ±0.00 -60.33 +60.33\t0.00 ±0.00 -0.00 +0.00\t0.00 ±0.00 -0.00 +0.00\t0.00 ±0.00 -0.00 +0.00\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/scikit_learn-0.20.3-py3.7-linux-x86_64.egg/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Combining features\n",
    "iterations = int(input(\"How many iterations: \"))\n",
    "heldoutset = int(input(\"Do you want to use a heldout set (1=heldout, 0=crossvalidate): \"))\n",
    "whichtype = int(input(\"Which type to test: \"))\n",
    "nuSVC = int(input(\"Should I use the nu-svc (1)?: \"))\n",
    "results = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    print()\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(\"Iteration \" + str(i+1) + \"/\" + str(iterations))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "    ### Selecting the actual training samples (Do some crossover here )\n",
    "    if heldoutset:\n",
    "        resulting_train_index = range(len(trainingdata))\n",
    "        resulting_test_index = range(len(testdata))\n",
    "        trainingdata_result = trainingdata.loc[resulting_train_index]\n",
    "        testdata_result = testdata.loc[resulting_test_index]\n",
    "    else:\n",
    "        ## Crossvalidating\n",
    "        resulting_train_index, resulting_test_index = helper.getindices(trainingdata, 4, 1) # Get 80-20\n",
    "        trainingdata_result = trainingdata.loc[resulting_train_index]\n",
    "        testdata_result = trainingdata.loc[resulting_test_index]\n",
    "    \n",
    "    final_train_x = []\n",
    "    final_test_x = []\n",
    "\n",
    "    if heldoutset:\n",
    "        ## BoW\n",
    "        if whichtype == 1:\n",
    "            final_train_x = train_bow[resulting_train_index]\n",
    "            final_test_x = test_bow[resulting_test_index]\n",
    "        ## Lexical\n",
    "        if whichtype == 2:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((test_bow[resulting_test_index], test_lexical_x[resulting_test_index]))\n",
    "        ## Sentiment\n",
    "        if whichtype == 4:\n",
    "            final_train_x = train_sentiment_x[resulting_train_index]\n",
    "            final_test_x = test_sentiment_x[resulting_test_index]\n",
    "        ## Word2Vec\n",
    "        if whichtype == 5:\n",
    "            final_train_x = word2vecfeatures_train[resulting_train_index]\n",
    "            final_test_x = word2vecfeatures_test[resulting_test_index]\n",
    "        ## Combined\n",
    "        if whichtype == 6:\n",
    "            final_train_x = np.hstack((word2vecfeatures_train[resulting_train_index],train_bow[resulting_train_index], train_sentiment_x[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((word2vecfeatures_test[resulting_test_index], test_bow[resulting_test_index], test_sentiment_x[resulting_test_index], test_lexical_x[resulting_test_index]))\n",
    "    else:\n",
    "        ## Crossvalidating\n",
    "        ## BoW\n",
    "        if whichtype == 1:\n",
    "            final_train_x = train_bow[resulting_train_index]\n",
    "            final_test_x = train_bow[resulting_test_index]\n",
    "        ## Lexical\n",
    "        if whichtype == 2:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((train_bow[resulting_test_index], x_lexical[resulting_test_index]))\n",
    "        ## Sentiment\n",
    "        if whichtype == 4:\n",
    "            final_train_x = train_sentiment_x[resulting_train_index]\n",
    "            final_test_x = train_sentiment_x[resulting_test_index]\n",
    "        ## Word2Vec\n",
    "        if whichtype == 5:\n",
    "            final_train_x = word2vecfeatures_train[resulting_train_index]\n",
    "            final_test_x = word2vecfeatures_train[resulting_test_index]\n",
    "        ## Combined\n",
    "        if whichtype == 6:\n",
    "            final_train_x = np.hstack((word2vecfeatures_train[resulting_train_index], train_bow[resulting_train_index], train_sentiment_x[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((word2vecfeatures_train[resulting_test_index], train_bow[resulting_test_index], train_sentiment_x[resulting_test_index], x_lexical[resulting_test_index]))\n",
    "\n",
    "\n",
    "    print(\"###########\")\n",
    "    print(\"Training:\")\n",
    "    print(final_train_x.shape)\n",
    "    print()\n",
    "    ### Train and get train error\n",
    "    y = trainingdata_result['Label'].tolist()\n",
    "    prob  = svm_problem(y, final_train_x)\n",
    "    if nuSVC:\n",
    "        param = svm_parameter('-t 2 -s 1 -c 8 -g ' + str(2**-11))\n",
    "    else:\n",
    "        param = svm_parameter('-t 2 -c 8 -g ' + str(2**-11))\n",
    "    trained_model = svm_train(prob, param)\n",
    "    p_label, p_acc, p_val = svm_predict(y, final_train_x, trained_model)\n",
    "    ACC, MSE, SCC = evaluations(y, p_label)\n",
    "\n",
    "    print()\n",
    "    print(\"###########\")\n",
    "    print(\"Testing:\")\n",
    "    print(final_test_x.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "    ### Get the test\n",
    "    test_y = testdata_result['Label'].tolist()\n",
    "\n",
    "    test_p_label, test_p_acc, test_p_val = svm_predict(test_y, final_test_x, trained_model)\n",
    "    test_ACC, test_MSE, test_SCC = evaluations(test_y, test_p_label)\n",
    "    print(sklearn.metrics.classification_report(test_y, test_p_label, digits=4))\n",
    "\n",
    "    if whichtask == \"A\":\n",
    "        p, r, f = helper.precision_recall_fscore(test_y, test_p_label, beta=1, labels=[0,1], pos_label=1)\n",
    "    elif whichtask == \"B\":\n",
    "        p, r, f = helper.precision_recall_fscore(test_y, test_p_label, beta=1, labels=[0,1,2,3])\n",
    "\n",
    "    print(\"Precision: \" + str(p))\n",
    "    print(\"Recall: \" + str(r))\n",
    "    print(\"F1-score: \" + str(f))\n",
    "    \n",
    "    results.append([test_ACC, p*100, r*100, f*100])\n",
    "\n",
    "    y_actu = pd.Series(test_y, name='Actual')\n",
    "    y_pred = pd.Series(test_p_label, name='Predicted')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(df_confusion)\n",
    "    \n",
    "\n",
    "### Write output\n",
    "mean_results = np.mean(np.array(results), axis=0)\n",
    "std_results = 2*np.std(np.array(results), axis=0)\n",
    "min_results = np.min(np.array(results), axis=0)\n",
    "max_results = np.max(np.array(results), axis=0)\n",
    "\n",
    "if whichtype == 1:\n",
    "    sys.stdout.write(\"BoW results\")\n",
    "elif whichtype == 2:\n",
    "    sys.stdout.write(\"Lexical results\")\n",
    "elif whichtype == 4:\n",
    "    sys.stdout.write(\"Semantic results\")\n",
    "elif whichtype == 5:\n",
    "    sys.stdout.write(\"Word2Vec results\")\n",
    "elif whichtype == 6:\n",
    "    sys.stdout.write(\"Combined results\")\n",
    "    \n",
    "### stdout\n",
    "print(\" with \" + str(iterations) + \" iterations: \")\n",
    "for result_i in range(len(mean_results)):\n",
    "    sys.stdout.write(\"{:.2f}\".format(mean_results[result_i]))\n",
    "    sys.stdout.write(\" ±\" + \"{:.2f}\".format(std_results[result_i]))\n",
    "    sys.stdout.write(\" -\" + \"{:.2f}\".format(min_results[result_i]))\n",
    "    sys.stdout.write(\" +\" + \"{:.2f}\".format(max_results[result_i]) + \"\\t\")\n",
    "    \n",
    "    \n",
    "### file out\n",
    "with open(\"Results/resultswrite.txt\", \"a\") as outfile:\n",
    "    if whichtype == 1:\n",
    "        outfile.write(\"BoW results\")\n",
    "    elif whichtype == 2:\n",
    "        outfile.write(\"Lexical results\")\n",
    "    elif whichtype == 4:\n",
    "        outfile.write(\"Semantic results\")\n",
    "    elif whichtype == 5:\n",
    "        outfile.write(\"Word2Vec results\")\n",
    "    elif whichtype == 6:\n",
    "        outfile.write(\"Combined results\")\n",
    "\n",
    "    ### stdout\n",
    "    outfile.write(\" with \" + str(iterations) + \" iterations: \\n\")\n",
    "    for result_i in range(len(mean_results)):\n",
    "        outfile.write(\"{:.2f}\".format(mean_results[result_i]))\n",
    "        outfile.write(\" ±\" + \"{:.2f}\".format(std_results[result_i]))\n",
    "        outfile.write(\" -\" + \"{:.2f}\".format(min_results[result_i]))\n",
    "        outfile.write(\" +\" + \"{:.2f}\".format(max_results[result_i]) + \"\\t\")\n",
    "        \n",
    "    outfile.write(\"\\n\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0 Not ironic\n",
      "473/473\n",
      "100.00%\n",
      "\n",
      "--- 1 Ironic by clash\n",
      "0/164\n",
      "0.00%\n",
      "\n",
      "--- 2 Situational irony\n",
      "0/85\n",
      "0.00%\n",
      "\n",
      "--- 3 Other irony\n",
      "0/62\n",
      "0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if heldoutset:\n",
    "    ### Evaluate results per category (Sectie 5.2)\n",
    "\n",
    "    testfileB = 'datasets/goldtest_TaskB/SemEval2018-T3_gold_test_taskB_emoji.txt'\n",
    "    testdataB = pd.read_csv(testfileB, sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "    testdataB = testdataB[['Label','Tweet text']]\n",
    "\n",
    "    ## Getting category information\n",
    "    used_testB = testdataB.loc[resulting_test_index]\n",
    "    used_testB.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    ## Nonirony\n",
    "    print(\"--- 0 Not ironic\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 0) & (y_pred == 0))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 0))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 1\n",
    "    print(\"--- 1 Ironic by clash\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 1) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 1))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 2\n",
    "    print(\"--- 2 Situational irony\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 2) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 2))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 3\n",
    "    print(\"--- 3 Other irony\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 3) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 3))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata_result = trainingdata.loc[resulting_train_index]\n",
    "testdata_result = trainingdata.loc[resulting_test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39668367346938777"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_result.index\n",
    "sum(testdata[\"Label\"] == 1) / len(testdata_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1923\n",
      "1911\n"
     ]
    }
   ],
   "source": [
    "print(sum([1 for yi in y if yi == 0]))\n",
    "print(sum([1 for yi in y if yi == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
