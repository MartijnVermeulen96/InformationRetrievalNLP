{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import twokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from FeatureFunctions import getfeatures\n",
    "import sklearn\n",
    "import csv\n",
    "\n",
    "\n",
    "### Import the classifier\n",
    "import sys\n",
    "sys.path.insert(0, 'libsvm')\n",
    "\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichtask = input(\"Which task to you want to do (A/B): \")\n",
    "whichtask = \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading train and test files\n",
    "\n",
    "datafile = \"datasets/train/SemEval2018-T3-train-task\"+whichtask+\"_emoji.txt\"\n",
    "# datafile2 = \"datasets/train/SemEval2018-T3-train-taskA_emoji_ironyHashtags.txt\"\n",
    "trainingdata = pd.read_csv(datafile, delimiter = \"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "trainingdata = trainingdata[['Label','Tweet text']]\n",
    "\n",
    "testfile = 'datasets/goldtest_Task'+whichtask+'/SemEval2018-T3_gold_test_task'+whichtask+'_emoji.txt'\n",
    "testdata = pd.read_csv(testfile, sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "testdata = testdata[['Label','Tweet text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get lexical features\n",
    "# training_features\n",
    "lexical_training_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(trainingdata, 'Tweet text')\n",
    "x_small = lexical_training_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "x_lexical = np.array(lexical_training_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "x_lexical = np.hstack((x_small, x_lexical))\n",
    "\n",
    "# train_bow = lexical_training_features['UnigramVector'].values.tolist()\n",
    "\n",
    "# test_features\n",
    "lexical_test_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(testdata, 'Tweet text', unicount_vect, bicount_vect, tricount_vect, fourcount_vect)\n",
    "test_x_small = lexical_test_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "test_lexical_x = np.array(lexical_test_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "test_lexical_x = np.hstack((test_x_small, test_lexical_x))\n",
    "\n",
    "train_bow = np.array(lexical_training_features['UnigramVector'].values.tolist())\n",
    "test_bow = np.array(lexical_test_features['UnigramVector'].values.tolist())\n",
    "\n",
    "lexical_training_features = []\n",
    "lexical_test_features = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+ only.\n",
    "reload(getfeatures)\n",
    "\n",
    "### Get sentiment features\n",
    "train_sentiment_x = getfeatures.getaffinfeats(trainingdata['Tweet text'])\n",
    "test_sentiment_x = getfeatures.getaffinfeats(testdata['Tweet text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getindices(trainingdata, testdata):\n",
    "    ### Creating 50-50% balance\n",
    "    ## Train\n",
    "\n",
    "    ### Count the amount of samples\n",
    "    amount_nonirony_train = sum(trainingdata[\"Label\"] == 0)\n",
    "    amount_irony_train = sum(trainingdata[\"Label\"] > 0)\n",
    "    amount_train_amount = min(amount_nonirony_train, amount_irony_train)\n",
    "\n",
    "    ### Sample indices\n",
    "    train_nonirony_index = trainingdata[trainingdata[\"Label\"] == 0].index.to_series()\n",
    "    train_irony_index = trainingdata[trainingdata[\"Label\"] > 0].index.to_series()\n",
    "\n",
    "    train_nonirony_index_samples = train_nonirony_index.sample(amount_train_amount).tolist()\n",
    "    train_irony_index_samples = train_irony_index.sample(amount_train_amount).tolist()\n",
    "\n",
    "    ## Test\n",
    "    amount_nonirony_test = sum(testdata[\"Label\"] == 0)\n",
    "    amount_irony_test = sum(testdata[\"Label\"] > 0)\n",
    "    amount_test_amount = min(amount_nonirony_test, amount_irony_test)\n",
    "\n",
    "    ### Sample indices\n",
    "    test_nonirony_index = testdata[testdata[\"Label\"] == 0].index.to_series()\n",
    "    test_irony_index = testdata[testdata[\"Label\"] > 0].index.to_series()\n",
    "\n",
    "    test_nonirony_index_samples = test_nonirony_index.sample(amount_test_amount).tolist()\n",
    "    test_irony_index_samples = test_irony_index.sample(amount_test_amount).tolist()\n",
    "    \n",
    "    resulting_train_index = train_nonirony_index_samples +train_irony_index_samples\n",
    "    resulting_test_index = test_nonirony_index_samples +test_irony_index_samples\n",
    "\n",
    "    return resulting_train_index, resulting_test_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations: 5\n",
      "Which type to test: 4\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1/5\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###########\n",
      "Training:\n",
      "(3822, 6)\n",
      "\n",
      "Accuracy = 58.7127% (2244/3822) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(622, 6)\n",
      "\n",
      "Accuracy = 56.1093% (349/622) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5495    0.6785    0.6072       311\n",
      "           1     0.5798    0.4437    0.5027       311\n",
      "\n",
      "   micro avg     0.5611    0.5611    0.5611       622\n",
      "   macro avg     0.5647    0.5611    0.5550       622\n",
      "weighted avg     0.5647    0.5611    0.5550       622\n",
      "\n",
      "Precision: 0.5798319327731093\n",
      "Recall: 0.4437299035369775\n",
      "F1-score: 0.5027322404371585\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          211  100  311\n",
      "1          173  138  311\n",
      "All        384  238  622\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 2/5\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###########\n",
      "Training:\n",
      "(3822, 6)\n",
      "\n",
      "Accuracy = 58.7127% (2244/3822) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(622, 6)\n",
      "\n",
      "Accuracy = 54.8232% (341/622) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5399    0.6527    0.5910       311\n",
      "           1     0.5610    0.4437    0.4955       311\n",
      "\n",
      "   micro avg     0.5482    0.5482    0.5482       622\n",
      "   macro avg     0.5504    0.5482    0.5432       622\n",
      "weighted avg     0.5504    0.5482    0.5432       622\n",
      "\n",
      "Precision: 0.5609756097560976\n",
      "Recall: 0.4437299035369775\n",
      "F1-score: 0.49551166965888693\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          203  108  311\n",
      "1          173  138  311\n",
      "All        376  246  622\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 3/5\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###########\n",
      "Training:\n",
      "(3822, 6)\n",
      "\n",
      "Accuracy = 58.7912% (2247/3822) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(622, 6)\n",
      "\n",
      "Accuracy = 54.0193% (336/622) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5337    0.6367    0.5806       311\n",
      "           1     0.5498    0.4437    0.4911       311\n",
      "\n",
      "   micro avg     0.5402    0.5402    0.5402       622\n",
      "   macro avg     0.5417    0.5402    0.5359       622\n",
      "weighted avg     0.5417    0.5402    0.5359       622\n",
      "\n",
      "Precision: 0.549800796812749\n",
      "Recall: 0.4437299035369775\n",
      "F1-score: 0.4911032028469751\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          198  113  311\n",
      "1          173  138  311\n",
      "All        371  251  622\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 4/5\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###########\n",
      "Training:\n",
      "(3822, 6)\n",
      "\n",
      "Accuracy = 58.6081% (2240/3822) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(622, 6)\n",
      "\n",
      "Accuracy = 53.0547% (330/622) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5262    0.6141    0.5668       311\n",
      "           1     0.5367    0.4469    0.4877       311\n",
      "\n",
      "   micro avg     0.5305    0.5305    0.5305       622\n",
      "   macro avg     0.5314    0.5305    0.5272       622\n",
      "weighted avg     0.5314    0.5305    0.5272       622\n",
      "\n",
      "Precision: 0.5366795366795367\n",
      "Recall: 0.44694533762057875\n",
      "F1-score: 0.48771929824561405\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          191  120  311\n",
      "1          172  139  311\n",
      "All        363  259  622\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 5/5\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "###########\n",
      "Training:\n",
      "(3822, 6)\n",
      "\n",
      "Accuracy = 58.6342% (2241/3822) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(622, 6)\n",
      "\n",
      "Accuracy = 54.9839% (342/622) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5413    0.6527    0.5918       311\n",
      "           1     0.5628    0.4469    0.4982       311\n",
      "\n",
      "   micro avg     0.5498    0.5498    0.5498       622\n",
      "   macro avg     0.5520    0.5498    0.5450       622\n",
      "weighted avg     0.5520    0.5498    0.5450       622\n",
      "\n",
      "Precision: 0.562753036437247\n",
      "Recall: 0.44694533762057875\n",
      "F1-score: 0.4982078853046594\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          203  108  311\n",
      "1          172  139  311\n",
      "All        375  247  622\n"
     ]
    }
   ],
   "source": [
    "### Combining features\n",
    "iterations = int(input(\"How many iterations: \"))\n",
    "whichtype = int(input(\"Which type to test: \"))\n",
    "\n",
    "for i in range(iterations):\n",
    "    print()\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(\"Iteration \" + str(i+1) + \"/\" + str(iterations))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "    ### Selecting the actual training samples (Do some crossover here )\n",
    "    resulting_train_index, resulting_test_index = getindices(trainingdata, testdata)\n",
    "    trainingdata_result = trainingdata.loc[resulting_train_index]\n",
    "    testdata_result = testdata.loc[resulting_test_index]\n",
    "    \n",
    "    final_train_x = []\n",
    "    final_test_x = []\n",
    "\n",
    "    ## BoW\n",
    "    if whichtype == 1:\n",
    "        final_train_x = train_bow[resulting_train_index]\n",
    "        final_test_x = test_bow[resulting_test_index]\n",
    "    ## Lexical\n",
    "    if whichtype == 2:\n",
    "        final_train_x = np.hstack((train_bow[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "        final_test_x = np.hstack((test_bow[resulting_test_index], test_lexical_x[resulting_test_index]))\n",
    "    ## Sentiment\n",
    "    if whichtype == 4:\n",
    "        final_train_x = train_sentiment_x[resulting_train_index]\n",
    "        final_test_x = test_sentiment_x[resulting_test_index]\n",
    "    ## Combined\n",
    "    if whichtype == 6:\n",
    "        final_train_x = np.hstack((train_bow[resulting_train_index], train_sentiment_x[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "        final_test_x = np.hstack((test_bow[resulting_test_index], test_sentiment_x[resulting_test_index], test_lexical_x[resulting_test_index]))\n",
    "\n",
    "\n",
    "    print(\"###########\")\n",
    "    print(\"Training:\")\n",
    "    print(final_train_x.shape)\n",
    "    print()\n",
    "    ### Train and get train error\n",
    "    y = trainingdata_result['Label'].tolist()\n",
    "    prob  = svm_problem(y, final_train_x)\n",
    "    param = svm_parameter('-t 2 -c 8 -g ' + str(2**-11))\n",
    "    m = svm_train(prob, param)\n",
    "    p_label, p_acc, p_val = svm_predict(y, final_train_x, m)\n",
    "    ACC, MSE, SCC = evaluations(y, p_label)\n",
    "\n",
    "    print()\n",
    "    print(\"###########\")\n",
    "    print(\"Testing:\")\n",
    "    print(final_test_x.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "    ### Get the test\n",
    "    test_y = testdata_result['Label'].tolist()\n",
    "\n",
    "    test_p_label, test_p_acc, test_p_val = svm_predict(test_y, final_test_x, m)\n",
    "    test_ACC, test_MSE, test_SCC = evaluations(test_y, test_p_label)\n",
    "    print(sklearn.metrics.classification_report(test_y, test_p_label, digits=4))\n",
    "\n",
    "    if whichtask == \"A\":\n",
    "        p, r, f = helper.precision_recall_fscore(test_y, test_p_label, beta=1, labels=[0,1], pos_label=1)\n",
    "    elif whichtask == \"B\":\n",
    "        p, r, f = helper.precision_recall_fscore(test_y, test_p_label, beta=1, labels=[0,1,2,3])\n",
    "\n",
    "    print(\"Precision: \" + str(p))\n",
    "    print(\"Recall: \" + str(r))\n",
    "    print(\"F1-score: \" + str(f))\n",
    "\n",
    "    y_actu = pd.Series(test_y, name='Actual')\n",
    "    y_pred = pd.Series(test_p_label, name='Predicted')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473\n",
      "311\n"
     ]
    }
   ],
   "source": [
    "print(amount_nonirony_test)\n",
    "print(amount_irony_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
