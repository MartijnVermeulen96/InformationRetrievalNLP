{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import twokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from FeatureFunctions import getfeatures\n",
    "from FeatureFunctions import helper\n",
    "\n",
    "import sklearn\n",
    "import csv\n",
    "\n",
    "\n",
    "### Import the classifier\n",
    "import sys\n",
    "sys.path.insert(0, 'libsvm')\n",
    "\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whichtask = input(\"Which task to you want to do (A/B): \")\n",
    "whichtask = \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading train and test files\n",
    "\n",
    "datafile = \"datasets/train/SemEval2018-T3-train-task\"+whichtask+\"_emoji.txt\"\n",
    "trainingdata = pd.read_csv(datafile, delimiter = \"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "trainingdata = trainingdata[['Label','Tweet text']]\n",
    "\n",
    "testfile = 'datasets/goldtest_Task'+whichtask+'/SemEval2018-T3_gold_test_task'+whichtask+'_emoji.txt'\n",
    "testdata = pd.read_csv(testfile, sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "testdata = testdata[['Label','Tweet text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get lexical features\n",
    "# training_features\n",
    "lexical_training_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(trainingdata, 'Tweet text')\n",
    "x_small = lexical_training_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "x_lexical = np.array(lexical_training_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "x_lexical = np.hstack((x_small, x_lexical))\n",
    "\n",
    "# train_bow = lexical_training_features['UnigramVector'].values.tolist()\n",
    "\n",
    "# test_features\n",
    "lexical_test_features, unicount_vect, bicount_vect, tricount_vect, fourcount_vect = getfeatures.getlexical(testdata, 'Tweet text', unicount_vect, bicount_vect, tricount_vect, fourcount_vect)\n",
    "test_x_small = lexical_test_features[['PunctuationFlood', 'CharFlood', 'CapitalizedCount', 'HashtagCount', 'Hashtag2WordRatio', 'TweetCharLength', 'TweetWordLength', 'EmojiCount', 'FinalPunctuation']].values\n",
    "test_lexical_x = np.array(lexical_test_features.apply(lambda row: sum([row['CharFourgramVector'], row['CharTrigramVector'],row['BigramVector']], []), axis=1).values.tolist())\n",
    "test_lexical_x = np.hstack((test_x_small, test_lexical_x))\n",
    "\n",
    "train_bow = np.array(lexical_training_features['UnigramVector'].values.tolist())\n",
    "test_bow = np.array(lexical_test_features['UnigramVector'].values.tolist())\n",
    "\n",
    "lexical_training_features = []\n",
    "lexical_test_features = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  # Python 3.4+ only.\n",
    "reload(getfeatures)\n",
    "\n",
    "### Get sentiment features\n",
    "train_sentiment_x = getfeatures.getaffinfeats(trainingdata['Tweet text'])\n",
    "test_sentiment_x = getfeatures.getaffinfeats(testdata['Tweet text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getindices(trainingdata, train_split=9, test_split=1):\n",
    "    ### Creating 50-50% balance\n",
    "    ## Train\n",
    "    \n",
    "    ### Count the amount of samples\n",
    "    amount_nonirony_train = sum(trainingdata[\"Label\"] == 0)\n",
    "    amount_irony_train = sum(trainingdata[\"Label\"] > 0)\n",
    "    amount_train_amount = min(amount_nonirony_train, amount_irony_train)\n",
    "    \n",
    "    \n",
    "\n",
    "    ### Sample indices\n",
    "    nonirony_index = trainingdata[trainingdata[\"Label\"] == 0].index.to_series()\n",
    "    irony_index = trainingdata[trainingdata[\"Label\"] > 0].index.to_series()\n",
    "\n",
    "    total_nonirony_samples = nonirony_index.sample(amount_train_amount).tolist()\n",
    "    total_irony_samples = irony_index.sample(amount_train_amount).tolist()\n",
    "    \n",
    "    train_amount = round(amount_train_amount / (train_split + test_split) * train_split)    \n",
    "    test_amount = round(amount_train_amount / (train_split + test_split) * test_split)    \n",
    "    \n",
    "    print(len(total_nonirony_samples))\n",
    "    print(len(total_irony_samples))\n",
    "\n",
    "    resulting_train_index = total_nonirony_samples[:train_amount] + total_irony_samples[:train_amount]\n",
    "    resulting_test_index = total_nonirony_samples[train_amount+1:] + total_irony_samples[train_amount+1:]\n",
    "\n",
    "    return resulting_train_index, resulting_test_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations: 50\n",
      "Do you want to use a heldout set (1=heldout, 0=crossvalidate): 0\n",
      "Which type to test: 2\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.9091% (2780/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 68.6352% (523/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6972    0.6588    0.6775       381\n",
      "           1     0.6766    0.7139    0.6948       381\n",
      "\n",
      "   micro avg     0.6864    0.6864    0.6864       762\n",
      "   macro avg     0.6869    0.6864    0.6861       762\n",
      "weighted avg     0.6869    0.6864    0.6861       762\n",
      "\n",
      "Precision: 0.6766169154228856\n",
      "Recall: 0.7139107611548556\n",
      "F1-score: 0.6947637292464879\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          251  130  381\n",
      "1          109  272  381\n",
      "All        360  402  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 2/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5494% (2769/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 68.3727% (521/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6923    0.6614    0.6765       381\n",
      "           1     0.6759    0.7060    0.6906       381\n",
      "\n",
      "   micro avg     0.6837    0.6837    0.6837       762\n",
      "   macro avg     0.6841    0.6837    0.6836       762\n",
      "weighted avg     0.6841    0.6837    0.6836       762\n",
      "\n",
      "Precision: 0.6758793969849246\n",
      "Recall: 0.7060367454068242\n",
      "F1-score: 0.6906290115532735\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          252  129  381\n",
      "1          112  269  381\n",
      "All        364  398  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 3/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.2551% (2760/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 67.979% (518/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6963    0.6378    0.6658       381\n",
      "           1     0.6659    0.7218    0.6927       381\n",
      "\n",
      "   micro avg     0.6798    0.6798    0.6798       762\n",
      "   macro avg     0.6811    0.6798    0.6792       762\n",
      "weighted avg     0.6811    0.6798    0.6792       762\n",
      "\n",
      "Precision: 0.6658595641646489\n",
      "Recall: 0.7217847769028871\n",
      "F1-score: 0.6926952141057935\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          243  138  381\n",
      "1          106  275  381\n",
      "All        349  413  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 4/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.1897% (2758/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.4226% (529/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7189    0.6378    0.6759       381\n",
      "           1     0.6745    0.7507    0.7106       381\n",
      "\n",
      "   micro avg     0.6942    0.6942    0.6942       762\n",
      "   macro avg     0.6967    0.6942    0.6932       762\n",
      "weighted avg     0.6967    0.6942    0.6932       762\n",
      "\n",
      "Precision: 0.6745283018867925\n",
      "Recall: 0.7506561679790026\n",
      "F1-score: 0.7105590062111801\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          243  138  381\n",
      "1           95  286  381\n",
      "All        338  424  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 5/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.6939% (2804/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 68.6352% (523/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6898    0.6772    0.6834       381\n",
      "           1     0.6830    0.6955    0.6892       381\n",
      "\n",
      "   micro avg     0.6864    0.6864    0.6864       762\n",
      "   macro avg     0.6864    0.6864    0.6863       762\n",
      "weighted avg     0.6864    0.6864    0.6863       762\n",
      "\n",
      "Precision: 0.6829896907216495\n",
      "Recall: 0.6955380577427821\n",
      "F1-score: 0.6892067620286085\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          258  123  381\n",
      "1          116  265  381\n",
      "All        374  388  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 6/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.7783% (2776/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.6037% (538/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7316    0.6509    0.6889       381\n",
      "           1     0.6856    0.7612    0.7214       381\n",
      "\n",
      "   micro avg     0.7060    0.7060    0.7060       762\n",
      "   macro avg     0.7086    0.7060    0.7051       762\n",
      "weighted avg     0.7086    0.7060    0.7051       762\n",
      "\n",
      "Precision: 0.6855791962174941\n",
      "Recall: 0.7611548556430446\n",
      "F1-score: 0.7213930348258706\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          248  133  381\n",
      "1           91  290  381\n",
      "All        339  423  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 7/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.9745% (2782/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 71.5223% (545/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7291    0.6850    0.7064       381\n",
      "           1     0.7030    0.7454    0.7236       381\n",
      "\n",
      "   micro avg     0.7152    0.7152    0.7152       762\n",
      "   macro avg     0.7160    0.7152    0.7150       762\n",
      "weighted avg     0.7160    0.7152    0.7150       762\n",
      "\n",
      "Precision: 0.7029702970297029\n",
      "Recall: 0.7454068241469817\n",
      "F1-score: 0.7235668789808918\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          261  120  381\n",
      "1           97  284  381\n",
      "All        358  404  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 8/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.1707% (2788/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 71.3911% (544/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7418    0.6562    0.6964       381\n",
      "           1     0.6918    0.7717    0.7295       381\n",
      "\n",
      "   micro avg     0.7139    0.7139    0.7139       762\n",
      "   macro avg     0.7168    0.7139    0.7130       762\n",
      "weighted avg     0.7168    0.7139    0.7130       762\n",
      "\n",
      "Precision: 0.691764705882353\n",
      "Recall: 0.7716535433070866\n",
      "F1-score: 0.7295285359801489\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          250  131  381\n",
      "1           87  294  381\n",
      "All        337  425  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 9/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.2878% (2761/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 73.3596% (559/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7392    0.7218    0.7304       381\n",
      "           1     0.7282    0.7454    0.7367       381\n",
      "\n",
      "   micro avg     0.7336    0.7336    0.7336       762\n",
      "   macro avg     0.7337    0.7336    0.7336       762\n",
      "weighted avg     0.7337    0.7336    0.7336       762\n",
      "\n",
      "Precision: 0.7282051282051282\n",
      "Recall: 0.7454068241469817\n",
      "F1-score: 0.7367055771725033\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          275  106  381\n",
      "1           97  284  381\n",
      "All        372  390  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 10/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.3532% (2763/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 69.4226% (529/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7139    0.6483    0.6795       381\n",
      "           1     0.6779    0.7402    0.7077       381\n",
      "\n",
      "   micro avg     0.6942    0.6942    0.6942       762\n",
      "   macro avg     0.6959    0.6942    0.6936       762\n",
      "weighted avg     0.6959    0.6942    0.6936       762\n",
      "\n",
      "Precision: 0.6778846153846154\n",
      "Recall: 0.7401574803149606\n",
      "F1-score: 0.7076537013801758\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          247  134  381\n",
      "1           99  282  381\n",
      "All        346  416  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 11/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.9745% (2782/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 71.1286% (542/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7230    0.6850    0.7035       381\n",
      "           1     0.7007    0.7375    0.7187       381\n",
      "\n",
      "   micro avg     0.7113    0.7113    0.7113       762\n",
      "   macro avg     0.7119    0.7113    0.7111       762\n",
      "weighted avg     0.7119    0.7113    0.7111       762\n",
      "\n",
      "Precision: 0.7007481296758105\n",
      "Recall: 0.7375328083989501\n",
      "F1-score: 0.7186700767263426\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          261  120  381\n",
      "1          100  281  381\n",
      "All        361  401  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 12/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.6802% (2773/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 71.1286% (542/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7230    0.6850    0.7035       381\n",
      "           1     0.7007    0.7375    0.7187       381\n",
      "\n",
      "   micro avg     0.7113    0.7113    0.7113       762\n",
      "   macro avg     0.7119    0.7113    0.7111       762\n",
      "weighted avg     0.7119    0.7113    0.7111       762\n",
      "\n",
      "Precision: 0.7007481296758105\n",
      "Recall: 0.7375328083989501\n",
      "F1-score: 0.7186700767263426\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          261  120  381\n",
      "1          100  281  381\n",
      "All        361  401  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 13/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.6802% (2773/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.4724% (537/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7378    0.6352    0.6827       381\n",
      "           1     0.6797    0.7743    0.7239       381\n",
      "\n",
      "   micro avg     0.7047    0.7047    0.7047       762\n",
      "   macro avg     0.7088    0.7047    0.7033       762\n",
      "weighted avg     0.7088    0.7047    0.7033       762\n",
      "\n",
      "Precision: 0.6797235023041475\n",
      "Recall: 0.7742782152230971\n",
      "F1-score: 0.7239263803680982\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          242  139  381\n",
      "1           86  295  381\n",
      "All        328  434  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 14/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 89.8627% (2748/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 68.1102% (519/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6927    0.6509    0.6712       381\n",
      "           1     0.6708    0.7113    0.6904       381\n",
      "\n",
      "   micro avg     0.6811    0.6811    0.6811       762\n",
      "   macro avg     0.6818    0.6811    0.6808       762\n",
      "weighted avg     0.6818    0.6811    0.6808       762\n",
      "\n",
      "Precision: 0.6707920792079208\n",
      "Recall: 0.7112860892388452\n",
      "F1-score: 0.6904458598726115\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          248  133  381\n",
      "1          110  271  381\n",
      "All        358  404  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 15/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.811% (2777/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.2913% (528/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7014    0.6719    0.6863       381\n",
      "           1     0.6851    0.7139    0.6992       381\n",
      "\n",
      "   micro avg     0.6929    0.6929    0.6929       762\n",
      "   macro avg     0.6933    0.6929    0.6928       762\n",
      "weighted avg     0.6933    0.6929    0.6928       762\n",
      "\n",
      "Precision: 0.6851385390428212\n",
      "Recall: 0.7139107611548556\n",
      "F1-score: 0.699228791773779\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          256  125  381\n",
      "1          109  272  381\n",
      "All        365  397  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 16/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.2224% (2759/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.685% (531/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7155    0.6535    0.6831       381\n",
      "           1     0.6812    0.7402    0.7094       381\n",
      "\n",
      "   micro avg     0.6969    0.6969    0.6969       762\n",
      "   macro avg     0.6983    0.6969    0.6963       762\n",
      "weighted avg     0.6983    0.6969    0.6963       762\n",
      "\n",
      "Precision: 0.6811594202898551\n",
      "Recall: 0.7401574803149606\n",
      "F1-score: 0.709433962264151\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          249  132  381\n",
      "1           99  282  381\n",
      "All        348  414  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 17/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.9745% (2782/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.3412% (536/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7034    0.7034    0.7034       381\n",
      "           1     0.7034    0.7034    0.7034       381\n",
      "\n",
      "   micro avg     0.7034    0.7034    0.7034       762\n",
      "   macro avg     0.7034    0.7034    0.7034       762\n",
      "weighted avg     0.7034    0.7034    0.7034       762\n",
      "\n",
      "Precision: 0.7034120734908137\n",
      "Recall: 0.7034120734908137\n",
      "F1-score: 0.7034120734908137\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          268  113  381\n",
      "1          113  268  381\n",
      "All        381  381  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 18/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5821% (2770/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 72.1785% (550/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7421    0.6798    0.7096       381\n",
      "           1     0.7046    0.7638    0.7330       381\n",
      "\n",
      "   micro avg     0.7218    0.7218    0.7218       762\n",
      "   macro avg     0.7234    0.7218    0.7213       762\n",
      "weighted avg     0.7234    0.7218    0.7213       762\n",
      "\n",
      "Precision: 0.7046004842615012\n",
      "Recall: 0.7637795275590551\n",
      "F1-score: 0.7329974811083124\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          259  122  381\n",
      "1           90  291  381\n",
      "All        349  413  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 19/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.9091% (2780/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.6037% (538/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7187    0.6772    0.6973       381\n",
      "           1     0.6948    0.7349    0.7143       381\n",
      "\n",
      "   micro avg     0.7060    0.7060    0.7060       762\n",
      "   macro avg     0.7067    0.7060    0.7058       762\n",
      "weighted avg     0.7067    0.7060    0.7058       762\n",
      "\n",
      "Precision: 0.6947890818858561\n",
      "Recall: 0.7349081364829396\n",
      "F1-score: 0.7142857142857142\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          258  123  381\n",
      "1          101  280  381\n",
      "All        359  403  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 20/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.3996% (2795/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 68.2415% (520/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6915    0.6588    0.6747       381\n",
      "           1     0.6742    0.7060    0.6897       381\n",
      "\n",
      "   micro avg     0.6824    0.6824    0.6824       762\n",
      "   macro avg     0.6828    0.6824    0.6822       762\n",
      "weighted avg     0.6828    0.6824    0.6822       762\n",
      "\n",
      "Precision: 0.6741854636591479\n",
      "Recall: 0.7060367454068242\n",
      "F1-score: 0.6897435897435898\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          251  130  381\n",
      "1          112  269  381\n",
      "All        363  399  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 21/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.9418% (2781/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.21% (535/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7115    0.6798    0.6953       381\n",
      "           1     0.6935    0.7244    0.7086       381\n",
      "\n",
      "   micro avg     0.7021    0.7021    0.7021       762\n",
      "   macro avg     0.7025    0.7021    0.7020       762\n",
      "weighted avg     0.7025    0.7021    0.7020       762\n",
      "\n",
      "Precision: 0.6934673366834171\n",
      "Recall: 0.7244094488188977\n",
      "F1-score: 0.7086007702182285\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          259  122  381\n",
      "1          105  276  381\n",
      "All        364  398  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 22/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.138% (2787/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.5538% (530/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6987    0.6877    0.6931       381\n",
      "           1     0.6925    0.7034    0.6979       381\n",
      "\n",
      "   micro avg     0.6955    0.6955    0.6955       762\n",
      "   macro avg     0.6956    0.6955    0.6955       762\n",
      "weighted avg     0.6956    0.6955    0.6955       762\n",
      "\n",
      "Precision: 0.6925064599483204\n",
      "Recall: 0.7034120734908137\n",
      "F1-score: 0.6979166666666666\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          262  119  381\n",
      "1          113  268  381\n",
      "All        375  387  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 23/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.1707% (2788/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.8163% (532/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7163    0.6562    0.6849       381\n",
      "           1     0.6828    0.7402    0.7103       381\n",
      "\n",
      "   micro avg     0.6982    0.6982    0.6982       762\n",
      "   macro avg     0.6996    0.6982    0.6976       762\n",
      "weighted avg     0.6996    0.6982    0.6976       762\n",
      "\n",
      "Precision: 0.6828087167070218\n",
      "Recall: 0.7401574803149606\n",
      "F1-score: 0.7103274559193954\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          250  131  381\n",
      "1           99  282  381\n",
      "All        349  413  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 24/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.9091% (2780/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 68.5039% (522/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6986    0.6509    0.6739       381\n",
      "           1     0.6732    0.7192    0.6954       381\n",
      "\n",
      "   micro avg     0.6850    0.6850    0.6850       762\n",
      "   macro avg     0.6859    0.6850    0.6847       762\n",
      "weighted avg     0.6859    0.6850    0.6847       762\n",
      "\n",
      "Precision: 0.6732186732186732\n",
      "Recall: 0.7191601049868767\n",
      "F1-score: 0.6954314720812184\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          248  133  381\n",
      "1          107  274  381\n",
      "All        355  407  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 25/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.2878% (2761/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 71.6535% (546/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7463    0.6562    0.6983       381\n",
      "           1     0.6932    0.7769    0.7327       381\n",
      "\n",
      "   micro avg     0.7165    0.7165    0.7165       762\n",
      "   macro avg     0.7197    0.7165    0.7155       762\n",
      "weighted avg     0.7197    0.7165    0.7155       762\n",
      "\n",
      "Precision: 0.6932084309133489\n",
      "Recall: 0.7769028871391076\n",
      "F1-score: 0.7326732673267325\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          250  131  381\n",
      "1           85  296  381\n",
      "All        335  427  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 26/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.4977% (2798/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 72.7034% (554/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7464    0.6877    0.7158       381\n",
      "           1     0.7105    0.7664    0.7374       381\n",
      "\n",
      "   micro avg     0.7270    0.7270    0.7270       762\n",
      "   macro avg     0.7285    0.7270    0.7266       762\n",
      "weighted avg     0.7285    0.7270    0.7266       762\n",
      "\n",
      "Precision: 0.7104622871046229\n",
      "Recall: 0.7664041994750657\n",
      "F1-score: 0.7373737373737373\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          262  119  381\n",
      "1           89  292  381\n",
      "All        351  411  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 27/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5494% (2769/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.9974% (541/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7247    0.6772    0.7001       381\n",
      "           1     0.6970    0.7428    0.7192       381\n",
      "\n",
      "   micro avg     0.7100    0.7100    0.7100       762\n",
      "   macro avg     0.7109    0.7100    0.7097       762\n",
      "weighted avg     0.7109    0.7100    0.7097       762\n",
      "\n",
      "Precision: 0.6970443349753694\n",
      "Recall: 0.7427821522309711\n",
      "F1-score: 0.7191867852604827\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          258  123  381\n",
      "1           98  283  381\n",
      "All        356  406  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 28/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.7129% (2774/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.3412% (536/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7112    0.6850    0.6979       381\n",
      "           1     0.6962    0.7218    0.7088       381\n",
      "\n",
      "   micro avg     0.7034    0.7034    0.7034       762\n",
      "   macro avg     0.7037    0.7034    0.7033       762\n",
      "weighted avg     0.7037    0.7034    0.7033       762\n",
      "\n",
      "Precision: 0.6962025316455697\n",
      "Recall: 0.7217847769028871\n",
      "F1-score: 0.7087628865979383\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          261  120  381\n",
      "1          106  275  381\n",
      "All        367  395  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 29/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.6802% (2773/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 68.1102% (519/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6816    0.6798    0.6807       381\n",
      "           1     0.6806    0.6824    0.6815       381\n",
      "\n",
      "   micro avg     0.6811    0.6811    0.6811       762\n",
      "   macro avg     0.6811    0.6811    0.6811       762\n",
      "weighted avg     0.6811    0.6811    0.6811       762\n",
      "\n",
      "Precision: 0.680628272251309\n",
      "Recall: 0.6824146981627297\n",
      "F1-score: 0.6815203145478376\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          259  122  381\n",
      "1          121  260  381\n",
      "All        380  382  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 30/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.8437% (2778/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 71.1286% (542/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7307    0.6693    0.6986       381\n",
      "           1     0.6949    0.7533    0.7229       381\n",
      "\n",
      "   micro avg     0.7113    0.7113    0.7113       762\n",
      "   macro avg     0.7128    0.7113    0.7108       762\n",
      "weighted avg     0.7128    0.7113    0.7108       762\n",
      "\n",
      "Precision: 0.6949152542372882\n",
      "Recall: 0.7532808398950132\n",
      "F1-score: 0.7229219143576827\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          255  126  381\n",
      "1           94  287  381\n",
      "All        349  413  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 31/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.2878% (2761/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.4226% (529/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7126    0.6509    0.6804       381\n",
      "           1     0.6787    0.7375    0.7069       381\n",
      "\n",
      "   micro avg     0.6942    0.6942    0.6942       762\n",
      "   macro avg     0.6957    0.6942    0.6937       762\n",
      "weighted avg     0.6957    0.6942    0.6937       762\n",
      "\n",
      "Precision: 0.678743961352657\n",
      "Recall: 0.7375328083989501\n",
      "F1-score: 0.7069182389937105\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          248  133  381\n",
      "1          100  281  381\n",
      "All        348  414  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 32/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5167% (2768/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 67.8478% (517/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6889    0.6509    0.6694       381\n",
      "           1     0.6692    0.7060    0.6871       381\n",
      "\n",
      "   micro avg     0.6785    0.6785    0.6785       762\n",
      "   macro avg     0.6790    0.6785    0.6782       762\n",
      "weighted avg     0.6790    0.6785    0.6782       762\n",
      "\n",
      "Precision: 0.6691542288557214\n",
      "Recall: 0.7060367454068242\n",
      "F1-score: 0.6871008939974457\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          248  133  381\n",
      "1          112  269  381\n",
      "All        360  402  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 33/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.138% (2787/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.685% (531/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7131    0.6588    0.6849       381\n",
      "           1     0.6829    0.7349    0.7080       381\n",
      "\n",
      "   micro avg     0.6969    0.6969    0.6969       762\n",
      "   macro avg     0.6980    0.6969    0.6964       762\n",
      "weighted avg     0.6980    0.6969    0.6964       762\n",
      "\n",
      "Precision: 0.6829268292682927\n",
      "Recall: 0.7349081364829396\n",
      "F1-score: 0.7079646017699116\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          251  130  381\n",
      "1          101  280  381\n",
      "All        352  410  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 34/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.6802% (2773/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 72.7034% (554/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7464    0.6877    0.7158       381\n",
      "           1     0.7105    0.7664    0.7374       381\n",
      "\n",
      "   micro avg     0.7270    0.7270    0.7270       762\n",
      "   macro avg     0.7285    0.7270    0.7266       762\n",
      "weighted avg     0.7285    0.7270    0.7266       762\n",
      "\n",
      "Precision: 0.7104622871046229\n",
      "Recall: 0.7664041994750657\n",
      "F1-score: 0.7373737373737373\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          262  119  381\n",
      "1           89  292  381\n",
      "All        351  411  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 35/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.7783% (2776/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.4724% (537/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7191    0.6719    0.6947       381\n",
      "           1     0.6921    0.7375    0.7141       381\n",
      "\n",
      "   micro avg     0.7047    0.7047    0.7047       762\n",
      "   macro avg     0.7056    0.7047    0.7044       762\n",
      "weighted avg     0.7056    0.7047    0.7044       762\n",
      "\n",
      "Precision: 0.6921182266009852\n",
      "Recall: 0.7375328083989501\n",
      "F1-score: 0.7141041931385006\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          256  125  381\n",
      "1          100  281  381\n",
      "All        356  406  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 36/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.0726% (2785/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.21% (535/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7175    0.6667    0.6912       381\n",
      "           1     0.6887    0.7375    0.7123       381\n",
      "\n",
      "   micro avg     0.7021    0.7021    0.7021       762\n",
      "   macro avg     0.7031    0.7021    0.7017       762\n",
      "weighted avg     0.7031    0.7021    0.7017       762\n",
      "\n",
      "Precision: 0.6887254901960784\n",
      "Recall: 0.7375328083989501\n",
      "F1-score: 0.7122940430925222\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          254  127  381\n",
      "1          100  281  381\n",
      "All        354  408  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 37/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 89.83% (2747/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.8163% (532/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7267    0.6352    0.6779       381\n",
      "           1     0.6760    0.7612    0.7160       381\n",
      "\n",
      "   micro avg     0.6982    0.6982    0.6982       762\n",
      "   macro avg     0.7014    0.6982    0.6970       762\n",
      "weighted avg     0.7014    0.6982    0.6970       762\n",
      "\n",
      "Precision: 0.675990675990676\n",
      "Recall: 0.7611548556430446\n",
      "F1-score: 0.7160493827160493\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          242  139  381\n",
      "1           91  290  381\n",
      "All        333  429  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 38/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5821% (2770/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.3412% (536/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7183    0.6693    0.6929       381\n",
      "           1     0.6904    0.7375    0.7132       381\n",
      "\n",
      "   micro avg     0.7034    0.7034    0.7034       762\n",
      "   macro avg     0.7044    0.7034    0.7031       762\n",
      "weighted avg     0.7044    0.7034    0.7031       762\n",
      "\n",
      "Precision: 0.6904176904176904\n",
      "Recall: 0.7375328083989501\n",
      "F1-score: 0.7131979695431472\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          255  126  381\n",
      "1          100  281  381\n",
      "All        355  407  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 39/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5167% (2768/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 72.3097% (551/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7361    0.6955    0.7152       381\n",
      "           1     0.7114    0.7507    0.7305       381\n",
      "\n",
      "   micro avg     0.7231    0.7231    0.7231       762\n",
      "   macro avg     0.7238    0.7231    0.7229       762\n",
      "weighted avg     0.7238    0.7231    0.7229       762\n",
      "\n",
      "Precision: 0.7114427860696517\n",
      "Recall: 0.7506561679790026\n",
      "F1-score: 0.7305236270753512\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          265  116  381\n",
      "1           95  286  381\n",
      "All        360  402  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 40/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.0399% (2784/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.8661% (540/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7239    0.6745    0.6984       381\n",
      "           1     0.6953    0.7428    0.7183       381\n",
      "\n",
      "   micro avg     0.7087    0.7087    0.7087       762\n",
      "   macro avg     0.7096    0.7087    0.7083       762\n",
      "weighted avg     0.7096    0.7087    0.7083       762\n",
      "\n",
      "Precision: 0.6953316953316954\n",
      "Recall: 0.7427821522309711\n",
      "F1-score: 0.7182741116751269\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          257  124  381\n",
      "1           98  283  381\n",
      "All        355  407  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 41/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.3532% (2763/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.8661% (540/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7252    0.6719    0.6975       381\n",
      "           1     0.6944    0.7454    0.7190       381\n",
      "\n",
      "   micro avg     0.7087    0.7087    0.7087       762\n",
      "   macro avg     0.7098    0.7087    0.7083       762\n",
      "weighted avg     0.7098    0.7087    0.7083       762\n",
      "\n",
      "Precision: 0.6943765281173594\n",
      "Recall: 0.7454068241469817\n",
      "F1-score: 0.7189873417721518\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          256  125  381\n",
      "1           97  284  381\n",
      "All        353  409  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 42/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 91.3342% (2793/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 67.5853% (515/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6683    0.6982    0.6829       381\n",
      "           1     0.6841    0.6535    0.6685       381\n",
      "\n",
      "   micro avg     0.6759    0.6759    0.6759       762\n",
      "   macro avg     0.6762    0.6759    0.6757       762\n",
      "weighted avg     0.6762    0.6759    0.6757       762\n",
      "\n",
      "Precision: 0.6840659340659341\n",
      "Recall: 0.6535433070866141\n",
      "F1-score: 0.6684563758389261\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          266  115  381\n",
      "1          132  249  381\n",
      "All        398  364  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 43/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.4513% (2766/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.5538% (530/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7211    0.6378    0.6769       381\n",
      "           1     0.6753    0.7533    0.7122       381\n",
      "\n",
      "   micro avg     0.6955    0.6955    0.6955       762\n",
      "   macro avg     0.6982    0.6955    0.6945       762\n",
      "weighted avg     0.6982    0.6955    0.6945       762\n",
      "\n",
      "Precision: 0.6752941176470588\n",
      "Recall: 0.7532808398950132\n",
      "F1-score: 0.7121588089330024\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          243  138  381\n",
      "1           94  287  381\n",
      "All        337  425  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 44/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.6802% (2773/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.4226% (529/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7022    0.6745    0.6881       381\n",
      "           1     0.6869    0.7139    0.7001       381\n",
      "\n",
      "   micro avg     0.6942    0.6942    0.6942       762\n",
      "   macro avg     0.6945    0.6942    0.6941       762\n",
      "weighted avg     0.6945    0.6942    0.6941       762\n",
      "\n",
      "Precision: 0.6868686868686869\n",
      "Recall: 0.7139107611548556\n",
      "F1-score: 0.7001287001287001\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          257  124  381\n",
      "1          109  272  381\n",
      "All        366  396  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 45/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5821% (2770/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.21% (535/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7238    0.6535    0.6869       381\n",
      "           1     0.6842    0.7507    0.7159       381\n",
      "\n",
      "   micro avg     0.7021    0.7021    0.7021       762\n",
      "   macro avg     0.7040    0.7021    0.7014       762\n",
      "weighted avg     0.7040    0.7021    0.7014       762\n",
      "\n",
      "Precision: 0.6842105263157895\n",
      "Recall: 0.7506561679790026\n",
      "F1-score: 0.7158948685857321\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          249  132  381\n",
      "1           95  286  381\n",
      "All        344  418  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 46/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.4186% (2765/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.6037% (538/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7151    0.6850    0.6997       381\n",
      "           1     0.6977    0.7270    0.7121       381\n",
      "\n",
      "   micro avg     0.7060    0.7060    0.7060       762\n",
      "   macro avg     0.7064    0.7060    0.7059       762\n",
      "weighted avg     0.7064    0.7060    0.7059       762\n",
      "\n",
      "Precision: 0.6977329974811083\n",
      "Recall: 0.7270341207349081\n",
      "F1-score: 0.712082262210797\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          261  120  381\n",
      "1          104  277  381\n",
      "All        365  397  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 47/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5821% (2770/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.4724% (537/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7191    0.6719    0.6947       381\n",
      "           1     0.6921    0.7375    0.7141       381\n",
      "\n",
      "   micro avg     0.7047    0.7047    0.7047       762\n",
      "   macro avg     0.7056    0.7047    0.7044       762\n",
      "weighted avg     0.7056    0.7047    0.7044       762\n",
      "\n",
      "Precision: 0.6921182266009852\n",
      "Recall: 0.7375328083989501\n",
      "F1-score: 0.7141041931385006\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          256  125  381\n",
      "1          100  281  381\n",
      "All        356  406  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 48/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.4186% (2765/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.5538% (530/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7099    0.6614    0.6848       381\n",
      "           1     0.6830    0.7297    0.7056       381\n",
      "\n",
      "   micro avg     0.6955    0.6955    0.6955       762\n",
      "   macro avg     0.6965    0.6955    0.6952       762\n",
      "weighted avg     0.6965    0.6955    0.6952       762\n",
      "\n",
      "Precision: 0.683046683046683\n",
      "Recall: 0.7296587926509186\n",
      "F1-score: 0.7055837563451777\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          252  129  381\n",
      "1          103  278  381\n",
      "All        355  407  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 49/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n",
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5821% (2770/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 70.21% (535/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6974    0.7139    0.7056       381\n",
      "           1     0.7070    0.6903    0.6985       381\n",
      "\n",
      "   micro avg     0.7021    0.7021    0.7021       762\n",
      "   macro avg     0.7022    0.7021    0.7021       762\n",
      "weighted avg     0.7022    0.7021    0.7021       762\n",
      "\n",
      "Precision: 0.706989247311828\n",
      "Recall: 0.6902887139107612\n",
      "F1-score: 0.6985391766268261\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          272  109  381\n",
      "1          118  263  381\n",
      "All        390  372  762\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Iteration 50/50\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "1911\n",
      "1911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########\n",
      "Training:\n",
      "(3058, 63411)\n",
      "\n",
      "Accuracy = 90.5494% (2769/3058) (classification)\n",
      "\n",
      "###########\n",
      "Testing:\n",
      "(762, 63411)\n",
      "\n",
      "Accuracy = 69.685% (531/762) (classification)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7060    0.6745    0.6899       381\n",
      "           1     0.6884    0.7192    0.7035       381\n",
      "\n",
      "   micro avg     0.6969    0.6969    0.6969       762\n",
      "   macro avg     0.6972    0.6969    0.6967       762\n",
      "weighted avg     0.6972    0.6969    0.6967       762\n",
      "\n",
      "Precision: 0.6884422110552764\n",
      "Recall: 0.7191601049868767\n",
      "F1-score: 0.7034659820282413\n",
      "\n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          257  124  381\n",
      "1          107  274  381\n",
      "All        364  398  762\n",
      "Lexical results with 50 iterations: \n",
      "70.11 ±2.64 -67.59 +73.36\t68.92 ±2.52 -66.59 +72.82\t73.28 ±4.98 -65.35 +77.69\t71.01 ±3.01 -66.85 +73.74\t\n"
     ]
    }
   ],
   "source": [
    "### Combining features\n",
    "iterations = int(input(\"How many iterations: \"))\n",
    "heldoutset = int(input(\"Do you want to use a heldout set (1=heldout, 0=crossvalidate): \"))\n",
    "whichtype = int(input(\"Which type to test: \"))\n",
    "results = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    print()\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(\"Iteration \" + str(i+1) + \"/\" + str(iterations))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "    ### Selecting the actual training samples (Do some crossover here )\n",
    "    if heldoutset:\n",
    "        trainingdata_result = trainingdata\n",
    "        testdata_result = testdata\n",
    "        resulting_train_index = range(len(trainingdata))\n",
    "        resulting_test_index = range(len(testdata_result))\n",
    "    else:\n",
    "        ## Crossvalidating\n",
    "        resulting_train_index, resulting_test_index = getindices(trainingdata, 4, 1) # Get 80-20\n",
    "        trainingdata_result = trainingdata.loc[resulting_train_index]\n",
    "        testdata_result = trainingdata.loc[resulting_test_index]\n",
    "    \n",
    "    final_train_x = []\n",
    "    final_test_x = []\n",
    "\n",
    "    if heldoutset:\n",
    "        ## BoW\n",
    "        if whichtype == 1:\n",
    "            final_train_x = train_bow[resulting_train_index]\n",
    "            final_test_x = test_bow[resulting_test_index]\n",
    "        ## Lexical\n",
    "        if whichtype == 2:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((test_bow[resulting_test_index], test_lexical_x[resulting_test_index]))\n",
    "        ## Sentiment\n",
    "        if whichtype == 4:\n",
    "            final_train_x = train_sentiment_x[resulting_train_index]\n",
    "            final_test_x = test_sentiment_x[resulting_test_index]\n",
    "        ## Combined\n",
    "        if whichtype == 6:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], train_sentiment_x[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((test_bow[resulting_test_index], test_sentiment_x[resulting_test_index], test_lexical_x[resulting_test_index]))\n",
    "    else:\n",
    "        ## Crossvalidating\n",
    "        ## BoW\n",
    "        if whichtype == 1:\n",
    "            final_train_x = train_bow[resulting_train_index]\n",
    "            final_test_x = train_bow[resulting_test_index]\n",
    "        ## Lexical\n",
    "        if whichtype == 2:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((train_bow[resulting_test_index], x_lexical[resulting_test_index]))\n",
    "        ## Sentiment\n",
    "        if whichtype == 4:\n",
    "            final_train_x = train_sentiment_x[resulting_train_index]\n",
    "            final_test_x = train_sentiment_x[resulting_test_index]\n",
    "        ## Combined\n",
    "        if whichtype == 6:\n",
    "            final_train_x = np.hstack((train_bow[resulting_train_index], train_sentiment_x[resulting_train_index], x_lexical[resulting_train_index]))\n",
    "            final_test_x = np.hstack((train_bow[resulting_test_index], train_sentiment_x[resulting_test_index], x_lexical[resulting_test_index]))\n",
    "\n",
    "\n",
    "    print(\"###########\")\n",
    "    print(\"Training:\")\n",
    "    print(final_train_x.shape)\n",
    "    print()\n",
    "    ### Train and get train error\n",
    "    y = trainingdata_result['Label'].tolist()\n",
    "    prob  = svm_problem(y, final_train_x)\n",
    "    param = svm_parameter('-t 2 -c 8 -g ' + str(2**-11))\n",
    "    m = svm_train(prob, param)\n",
    "    p_label, p_acc, p_val = svm_predict(y, final_train_x, m)\n",
    "    ACC, MSE, SCC = evaluations(y, p_label)\n",
    "\n",
    "    print()\n",
    "    print(\"###########\")\n",
    "    print(\"Testing:\")\n",
    "    print(final_test_x.shape)\n",
    "    print()\n",
    "\n",
    "\n",
    "    ### Get the test\n",
    "    test_y = testdata_result['Label'].tolist()\n",
    "\n",
    "    test_p_label, test_p_acc, test_p_val = svm_predict(test_y, final_test_x, m)\n",
    "    test_ACC, test_MSE, test_SCC = evaluations(test_y, test_p_label)\n",
    "    print(sklearn.metrics.classification_report(test_y, test_p_label, digits=4))\n",
    "\n",
    "    if whichtask == \"A\":\n",
    "        p, r, f = helper.precision_recall_fscore(test_y, test_p_label, beta=1, labels=[0,1], pos_label=1)\n",
    "    elif whichtask == \"B\":\n",
    "        p, r, f = helper.precision_recall_fscore(test_y, test_p_label, beta=1, labels=[0,1,2,3])\n",
    "\n",
    "    print(\"Precision: \" + str(p))\n",
    "    print(\"Recall: \" + str(r))\n",
    "    print(\"F1-score: \" + str(f))\n",
    "    \n",
    "    results.append([test_ACC, p*100, r*100, f*100])\n",
    "\n",
    "    y_actu = pd.Series(test_y, name='Actual')\n",
    "    y_pred = pd.Series(test_p_label, name='Predicted')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(df_confusion)\n",
    "    \n",
    "\n",
    "### Write output\n",
    "mean_results = np.mean(np.array(results), axis=0)\n",
    "std_results = 2*np.std(np.array(results), axis=0)\n",
    "min_results = np.min(np.array(results), axis=0)\n",
    "max_results = np.max(np.array(results), axis=0)\n",
    "\n",
    "if whichtype == 1:\n",
    "    sys.stdout.write(\"BoW results\")\n",
    "elif whichtype == 2:\n",
    "    sys.stdout.write(\"Lexical results\")\n",
    "elif whichtype == 4:\n",
    "    sys.stdout.write(\"Semantic results\")\n",
    "elif whichtype == 6:\n",
    "    sys.stdout.write(\"Combined results\")\n",
    "    \n",
    "### stdout\n",
    "print(\" with \" + str(iterations) + \" iterations: \")\n",
    "for result_i in range(len(mean_results)):\n",
    "    sys.stdout.write(\"{:.2f}\".format(mean_results[result_i]))\n",
    "    sys.stdout.write(\" ±\" + \"{:.2f}\".format(std_results[result_i]))\n",
    "    sys.stdout.write(\" -\" + \"{:.2f}\".format(min_results[result_i]))\n",
    "    sys.stdout.write(\" +\" + \"{:.2f}\".format(max_results[result_i]) + \"\\t\")\n",
    "    \n",
    "    \n",
    "### file out\n",
    "with open(\"Results/resultswrite.txt\", \"a\") as outfile:\n",
    "    if whichtype == 1:\n",
    "        outfile.write(\"BoW results\")\n",
    "    elif whichtype == 2:\n",
    "        outfile.write(\"Lexical results\")\n",
    "    elif whichtype == 4:\n",
    "        outfile.write(\"Semantic results\")\n",
    "    elif whichtype == 6:\n",
    "        outfile.write(\"Combined results\")\n",
    "\n",
    "    ### stdout\n",
    "    outfile.write(\" with \" + str(iterations) + \" iterations: \\n\")\n",
    "    for result_i in range(len(mean_results)):\n",
    "        outfile.write(\"{:.2f}\".format(mean_results[result_i]))\n",
    "        outfile.write(\" ±\" + \"{:.2f}\".format(std_results[result_i]))\n",
    "        outfile.write(\" -\" + \"{:.2f}\".format(min_results[result_i]))\n",
    "        outfile.write(\" +\" + \"{:.2f}\".format(max_results[result_i]) + \"\\t\")\n",
    "        \n",
    "    outfile.write(\"\\n\\n\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if heldoutset:\n",
    "    ### Evaluate results per category (Sectie 5.2)\n",
    "\n",
    "    testfileB = 'datasets/goldtest_TaskB/SemEval2018-T3_gold_test_taskB_emoji.txt'\n",
    "    testdataB = pd.read_csv(testfileB, sep=\"\\t\",  quoting=csv.QUOTE_NONE, header=0)\n",
    "    testdataB = testdataB[['Label','Tweet text']]\n",
    "\n",
    "    ## Getting category information\n",
    "    used_testB = testdataB.loc[resulting_test_index]\n",
    "    used_testB.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    ## Nonirony\n",
    "    print(\"--- 0 Not ironic\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 0) & (y_pred == 0))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 0))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 1\n",
    "    print(\"--- 1 Ironic by clash\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 1) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 1))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 2\n",
    "    print(\"--- 2 Situational irony\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 2) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 2))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n",
    "\n",
    "    ## 3\n",
    "    print(\"--- 3 Other irony\")\n",
    "    predB_nonirony = sum((used_testB['Label'] == 3) & (y_pred == 1))\n",
    "    testB_nonirony = sum((used_testB['Label'] == 3))\n",
    "    print(str(predB_nonirony) + \"/\" + str(testB_nonirony))\n",
    "    print(\"{:.2f}%\".format(predB_nonirony/testB_nonirony * 100))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata_result = trainingdata.loc[resulting_train_index]\n",
    "testdata_result = trainingdata.loc[resulting_test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40813648293963256"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_result.index\n",
    "sum(testdata[\"Label\"] == 1) / len(testdata_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1529\n",
      "1529\n"
     ]
    }
   ],
   "source": [
    "print(sum([1 for yi in y if yi == 0]))\n",
    "print(sum([1 for yi in y if yi == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
