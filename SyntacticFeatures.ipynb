{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import twokenize\n",
    "import arktweetnlpmaster\n",
    "\n",
    "datafile2 = \"datasets/train/SemEval2018-T3-train-taskA_emoji_ironyHashtags.txt\"\n",
    "trainingdata = pd.read_csv(datafile2, sep = \"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata = trainingdata[['Label','Tweet text']]\n",
    "tweets = trainingdata['Tweet text']\n",
    "allbags = []\n",
    "for i in range(len(trainingdata)):\n",
    "    bagofwords = twokenize.simpleTokenize(tweets.iloc[i])\n",
    "    allbags.append(bagofwords)\n",
    "trainingdata[\"Bag\"] = allbags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = CMUTweetTagger.runtagger_parse(['example tweet 1', 'example tweet 2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module twokenize:\n",
      "\n",
      "NAME\n",
      "    twokenize\n",
      "\n",
      "DESCRIPTION\n",
      "    Twokenize -- a tokenizer designed for Twitter text in English and some other European languages.\n",
      "    This tokenizer code has gone through a long history:\n",
      "    \n",
      "    (1) Brendan O'Connor wrote original version in Python, http://github.com/brendano/tweetmotif\n",
      "           TweetMotif: Exploratory Search and Topic Summarization for Twitter.\n",
      "           Brendan O'Connor, Michel Krieger, and David Ahn.\n",
      "           ICWSM-2010 (demo track), http://brenocon.com/oconnor_krieger_ahn.icwsm2010.tweetmotif.pdf\n",
      "    (2a) Kevin Gimpel and Daniel Mills modified it for POS tagging for the CMU ARK Twitter POS Tagger\n",
      "    (2b) Jason Baldridge and David Snyder ported it to Scala\n",
      "    (3) Brendan bugfixed the Scala port and merged with POS-specific changes\n",
      "        for the CMU ARK Twitter POS Tagger\n",
      "    (4) Tobi Owoputi ported it back to Java and added many improvements (2012-06)\n",
      "    \n",
      "    Current home is http://github.com/brendano/ark-tweet-nlp and http://www.ark.cs.cmu.edu/TweetNLP\n",
      "    \n",
      "    There have been at least 2 other Java ports, but they are not in the lineage for the code here.\n",
      "    \n",
      "    Ported to Python by Myle Ott <myleott@gmail.com>.\n",
      "\n",
      "FUNCTIONS\n",
      "    addAllnonempty(master, smaller)\n",
      "    \n",
      "    normalizeTextForTagger(text)\n",
      "        # Twitter text comes HTML-escaped, so unescape it.\n",
      "        # We also first unescape &amp;'s, in case the text has been buggily double-escaped.\n",
      "    \n",
      "    regex_or(*items)\n",
      "    \n",
      "    simpleTokenize(text)\n",
      "        # The main work of tokenizing a tweet.\n",
      "    \n",
      "    splitEdgePunct(input)\n",
      "    \n",
      "    splitToken(token)\n",
      "        # Final pass tokenization based on special patterns\n",
      "    \n",
      "    squeezeWhitespace(input)\n",
      "        # \"foo   bar \" => \"foo bar\"\n",
      "    \n",
      "    tokenize(text)\n",
      "        # Assume 'text' has no HTML escaping.\n",
      "    \n",
      "    tokenizeRawTweetText(text)\n",
      "        # This is intended for raw tweet text -- we do some HTML entity unescaping before running the tagger.\n",
      "        #\n",
      "        # This function normalizes the input text BEFORE calling the tokenizer.\n",
      "        # So the tokens you get back may not exactly correspond to\n",
      "        # substrings of the original text.\n",
      "\n",
      "DATA\n",
      "    Arrows = '(?:(?:<*[-―—=]*>+|<+[-―—=]*>*)|[←-⇿]+)'\n",
      "    AtMention = '[@＠][a-zA-Z0-9_]+'\n",
      "    Bound = r'(?:\\W|^|$)'\n",
      "    Contractions = re.compile(\"(?i)(\\\\w+)(n['’′]t|['’′]ve|['’′]ll|['’′]d|[...\n",
      "    EdgePunctLeft = re.compile('(^|$|:|;|\\\\s|\\\\.|,)([\\'\"“”‘’«»{}\\\\(\\\\)\\\\[\\...\n",
      "    EdgePunctRight = re.compile('([a-zA-Z0-9])([\\'\"“”‘’«»{}\\\\(\\\\)\\\\[\\\\]\\\\*...\n",
      "    Email = r'(?:(?<=(?:\\W))|(?<=(?:^)))[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\....\n",
      "    Hashtag = '#[a-zA-Z0-9_]+'\n",
      "    Hearts = '(?:<+/?3+)+'\n",
      "    Protected = re.compile('(?:(?:<+/?3+)+|(?:(?:https?://|\\\\bww...jobs|mo...\n",
      "    Whitespace = re.compile('[\\\\s \\xa0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000...\n",
      "    aa1 = r'(?:[A-Za-z]\\.){2,}(?=(?:$|\\s|[“\\\"?!,:;]|&(?:amp|lt|gt|quot);))...\n",
      "    aa2 = r'[^A-Za-z](?:[A-Za-z]\\.){1,}[A-Za-z](?=(?:$|\\s|[“\\\"?!,:;]|&(?:a...\n",
      "    arbitraryAbbrev = r'(?:(?:[A-Za-z]\\.){2,}(?=(?:$|\\s|[“\\\"?!,:;]|&(?:a.....\n",
      "    basicface = r'(?:(♥|0|[oO]|°|[vV]|\\$|[tT]|[xX]|;|ಠ|@|ʘ|•|・|◕|\\...t;|>|...\n",
      "    bfCenter = r'(?:[\\.]|[_-]+)'\n",
      "    bfLeft = r'(♥|0|[oO]|°|[vV]|\\$|[tT]|[xX]|;|ಠ|@|ʘ|•|・|◕|\\^|¬|\\*)'\n",
      "    bfRight = r'\\2'\n",
      "    boundaryNotDot = r'(?:$|\\s|[“\\\"?!,:;]|&(?:amp|lt|gt|quot);)'\n",
      "    ccTLDs = '(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|...|uy|uz|va...\n",
      "    commonTLDs = '(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|i...\n",
      "    decorations = '(?:[♫♪]+|[★☆]+|[♥❤♡]+|[☹-☻]+|[\\ue001-\\uebbb]+)'\n",
      "    eastEmote = '[＼\\\\\\\\ƪԄ\\\\(（<>;ヽ\\\\-=~\\\\*]+(?:(?:(♥|0|[oO]|°|[vV]|\\\\$|[......\n",
      "    edgePunct = '[\\'\"“”‘’«»{}\\\\(\\\\)\\\\[\\\\]\\\\*&]'\n",
      "    edgePunctChars = '\\'\"“”‘’«»{}\\\\(\\\\)\\\\[\\\\]\\\\*&'\n",
      "    eeLeft = r'[＼\\\\ƪԄ\\(（<>;ヽ\\-=~\\*]+'\n",
      "    eeRight = '[\\\\-=\\\\);\\'\"<>ʃ）/／ノﾉ丿╯σっµ~\\\\*]+'\n",
      "    eeSymbol = r'[^A-Za-z0-9\\s\\(\\)\\*:=-]'\n",
      "    embeddedApostrophe = '[^\\\\s\\\\.,?\\\\\"]+[\\'’′][^\\\\s\\\\.,?\\\\\"]*'\n",
      "    emoticon = r'(?:(?:>|&gt;)?(?:[:=]|[;])(?:(?:|-|[^a-zA-Z0-9 ]...|&gt;)...\n",
      "    entity = '&(?:amp|lt|gt|quot);'\n",
      "    happyMouths = r'[D\\)\\]\\}]+'\n",
      "    normalEyes = '[:=]'\n",
      "    noseArea = '(?:|-|[^a-zA-Z0-9 ])'\n",
      "    notEdgePunct = '[a-zA-Z0-9]'\n",
      "    numComb = r'[$֏؋৲৳৻૱௹฿៛꠸﷼﹩＄￠￡￥￦¢-¥₠-₹]?\\d+(?:\\.\\d+)+%?'\n",
      "    numberWithCommas = r'(?:(?<!\\d)\\d{1,3},)+?\\d{3}(?=(?:[^,\\d]|$))'\n",
      "    oOEmote = r'(?:[oO](?:[\\.]|[_-]+)[oO])'\n",
      "    offEdge = r'(^|$|:|;|\\s|\\.|,)'\n",
      "    otherMouths = r'(?:[oO]+|[/\\\\]+|[vV]+|[Ss]+|[|]+)'\n",
      "    punctChars = '[\\'\\\\\"“”‘’.?!…,:;]'\n",
      "    punctSeq = '[\\'\\\\\"“”‘’]+|[.?!,…]+|[:;]+'\n",
      "    s3 = '(?:--[\\'\\\\\"])'\n",
      "    s4 = r'(?:<|&lt;|>|&gt;)[\\._-]+(?:<|&lt;|>|&gt;)'\n",
      "    s5 = '(?:[.][_]+[.])'\n",
      "    sadMouths = r'[\\(\\[\\{]+'\n",
      "    separators = '(?:--+|―|—|~|–|=)'\n",
      "    standardAbbreviations = r'\\b(?:[Mm]r|[Mm]rs|[Mm]s|[Dd]r|[Ss]r|[Jj]r|[R...\n",
      "    thingsThatSplitWords = r'[^\\s\\.,?\\\"]'\n",
      "    timeLike = r'\\d+(?::\\d+){1,2}'\n",
      "    tongue = '[pPd3]+'\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    url = r'(?:(?:https?://|\\bwww\\.)|\\b(?:[A-Za-z\\d-])+(?:\\....:;]|&(?:amp...\n",
      "    urlBody = r'(?:[^\\.\\s<>][^\\s<>]*?)?'\n",
      "    urlEnd = r'(?:\\.\\.+|[<>]|\\s|$)'\n",
      "    urlExtraCrapBeforeEnd = '(?:[\\'\\\\\"“”‘’.?!…,:;]|&(?:amp|lt|gt|quot);)+?...\n",
      "    urlStart1 = r'(?:https?://|\\bwww\\.)'\n",
      "    urlStart2 = r'\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.(?:(?...vc|ve...\n",
      "    wink = '[;]'\n",
      "\n",
      "FILE\n",
      "    /Users/sebastiaanscholten/Documents/InformationRetrievalNLP/twokenize.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(twokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
